---
title: Pandas/Geopandas Tricks
slug: pandas-geopandas-tricks
date: 2020-01-13T09:01:39.000Z
pinned: true
categories:
  - æ”¶è—
tags:
  - Python
  - Pandas
  - GeoPandas
  - ä¼˜åŒ–
lastmod: '2021-07-06T01:21:07.606Z'
---

> æ€»ç»“ä¸ªäººä½¿ç”¨ä¸­å¸¸ç”¨PandasåŠæ‰©å±•æ’ä»¶ä½¿ç”¨æŠ€å·§


<!--more-->



### I/O
#### pandaså¯ä»¥ç›´æ¥è¯»å–å‹ç¼©æ–‡ä»¶ï¼ŒåŒæ ·å†™å¯ä»¥å†™å…¥å‹ç¼©æ–‡ä»¶
[å‚è€ƒ](https://twitter.com/justmarkham/status/1146764820697505792)
You can read directly from a compressed file, Or write to a compressed file. 
Also supported: .gz, .bz2, .xz

#### HDFStore
å°½å¯èƒ½çš„é¿å…è¯»å–åŸå§‹csvï¼Œä½¿ç”¨hdfã€featheræˆ–h5pyæ ¼å¼æ–‡ä»¶åŠ å¿«æ–‡ä»¶è¯»å– ([å‚è€ƒ1](https://zhuanlan.zhihu.com/p/81554435), [å‚è€ƒ2](https://www.cnblogs.com/feffery/p/11135082.html))
HDF5ï¼ˆHierarchical Data Formalï¼‰æ˜¯ç”¨äºå­˜å‚¨å¤§è§„æ¨¡æ•°å€¼æ•°æ®çš„è¾ƒä¸ºç†æƒ³çš„å­˜å‚¨æ ¼å¼ï¼Œæ–‡ä»¶åç¼€åä¸ºh5ï¼Œå­˜å‚¨è¯»å–é€Ÿåº¦éå¸¸å¿«ï¼Œä¸”å¯åœ¨æ–‡ä»¶å†…éƒ¨æŒ‰ç…§æ˜ç¡®çš„å±‚æ¬¡å­˜å‚¨æ•°æ®ï¼ŒåŒä¸€ä¸ªHDF5å¯ä»¥çœ‹åšä¸€ä¸ªé«˜åº¦æ•´åˆçš„æ–‡ä»¶å¤¹ï¼Œå…¶å†…éƒ¨å¯å­˜æ”¾ä¸åŒç±»å‹çš„æ•°æ®ã€‚
```Python
import pandas as pd
import numpy as np

# åˆ›å»ºæ–°çš„å¯¹è±¡ã€è¯»å…¥å·²å­˜åœ¨çš„å¯¹è±¡
store = pd.HDFStore('demo.h5')

# å¯¼å‡ºåˆ°å·²å­˜åœ¨çš„h5æ–‡ä»¶ä¸­ï¼Œè¿™é‡Œéœ€è¦æŒ‡å®škey
df_.to_hdf(path_or_buf='demo.h5',key='df_')


s = pd.Series(np.random.randn(5), index=["a", "b", "c", "d", "e"])
df = pd.DataFrame(np.random.randn(8, 3), columns=["A", "B", "C"])
# å°† Series æˆ– DataFrame å­˜å…¥ store
store["s"], store["df"] = s, df

# æŸ¥çœ‹ store ä¸­æœ‰å“ªäº›æ•°æ®
store.keys()
# out: ['/df', '/s']

# å–å‡ºæŸä¸€æ•°æ®
df = store["df"]

# åˆ é™¤storeå¯¹è±¡ä¸­æŒ‡å®šæ•°æ®
del store['s']

# å°†å½“å‰çš„storeå¯¹è±¡æŒä¹…åŒ–åˆ°æœ¬åœ°
store.close()

# æŸ¥çœ‹è¿æ¥çŠ¶å†µ
store.is_open
```
HDF5ç”¨æ—¶ä»…ä¸ºcsvçš„1/13ï¼Œå› æ­¤åœ¨æ¶‰åŠåˆ°æ•°æ®å­˜å‚¨ç‰¹åˆ«æ˜¯è§„æ¨¡è¾ƒå¤§çš„æ•°æ®æ—¶ï¼ŒHDF5æ˜¯ä½ ä¸é”™çš„é€‰æ‹©ã€‚

#### è¯»å–csv
```Python
one_piece_df = pd.read_csv(csv_path, header = 0, encoding='gbk', engine='python', error_bad_lines=False)
```
- encoding: ç¼–ç é—®é¢˜
- **engine**:  æŠ¥é”™- ParserError: Error tokenizing data. C error: EOF inside string starting at row 15946
- **error_bad_lines: å¿½ç•¥æœ‰é”™è¯¯çš„è¡Œ, è¿™ä¸ªç”¨å¤„æ¯”è¾ƒå¤§ï¼Œæœ‰å¾ˆå¤šç±»å‹çš„æŠ¥é”™éƒ½å¯ä»¥è§£å†³ï¼Œå»ºè®®ä¸€èˆ¬æƒ…å†µä¸‹åŠ ä¸Š**: Skipping line 15513: â€™ â€™ expected after â€˜"â€™; Skipping line 15546: unexpected end of data; ParserError: Expected 19 fields in line 212, saw 20field larger than field limit (131072)


#### ä¿å­˜ä¸ºjson
```Python
# å»ºè®®ä¿å­˜æ–¹æ³•:
parcels_info_df.to_json('ParcelsInfo.json', orient='indexâ€™)
# åŒæ ·è¯»å–æ–¹æ³•ï¼š
pd.read_json('ParcelsInfo.json', orient='indexâ€™)
```
å…¶ä»–ä¿å­˜æ–¹æ³•ï¼š
```Python
with open('./name.json', 'w', encoding='utf-8') as fp:
    json.dump(result_dict, fp, indent=4)
```

#### DataFrame and Dict:
```Python
# Dict 2 DataFrame:
kmeans_result_df = pd.DataFrame.from_dict(kmeans_result_dict)
# DataFrame 2 Dict:
kmeans_result_dict = pd.DataFrame.to_dict(kmeans_result_df)
```

#### List of dict and DataFrame
```Python
# List of dict to DataFrame
data_list = [{'points': 50, 'time': '5:00', 'year': 2010}, 
             {'points': 25, 'time': '6:00', 'month': "february"}, 
             {'points':90, 'time': '9:00', 'month': 'january'}, 
             {'points_h1':20, 'month': 'june'}]
df = pd.DataFrame(data_list)
# DataFrame to List of dict
data_list = df.T.to_dict().values()
```


#### dict2nametuple
```python
from collections import namedtuple
args_dict = {
    'no_cuda': False, 
    'fastmode': False, 
    'seed': 666, 
    'epochs': 500,
    'lr': 0.01, 
    'weight_decay': 5e-4, 
    'hidden': 64, 
    'dropout': 0.5,
}
Args = namedtuple('Args', [_ for _ in args_dict.keys()])
args = Args(**(args_dict))
```

#### DataFrameå¯¼å‡ºMarkdown
```python
from tabulate import tabulate

df = DataFrame({
    "weekday": ["monday", "thursday", "wednesday"],
    "temperature": [20, 30, 25],
    "precipitation": [100, 200, 150],
}).set_index("weekday")

md = tabulate(df, tablefmt="pipe", headers="keys")
print(md)
```

#### joblib
```python
# ä¿å­˜å˜é‡
metrics_fp = Path(gensim_model_dir, 'metrics.dat')
joblib.dump(value=metric_list, filename=str(metrics_fp))

# è½½å…¥å˜é‡
metrics_fp = Path(gensim_model_dir, 'metrics.dat')
# metric_list = joblib.load(metrics_fp)
```

### æ•°æ®åº“äº¤äº’

#### postgresqläº¤äº’
```python
from sqlalchemy import create_engine
from geoalchemy2 import Geometry, WKTElement

import pandas as pd
import geopandas as gpd

'''
Geopanda, pandas 2 postgresql
postgisæ“ä½œåœ¨å»ºç«‹æ•°æ®åº“åéœ€æ·»åŠ postgisæ‰©å±•ï¼Œå¯åœ¨pgAdminä¸­æ–°å»ºæ•°æ®åº“åæ·»åŠ 
'''


class Transit(object):
    def __init__(self, engine_string, dbschema):
        self.engine_string = engine_string
        self.dbschema = dbschema
        self.engine = self.connect()

    def connect(self):
        engine = create_engine(
            self.engine_string,
            use_batch_mode=True,
            connect_args={'options': '-csearch_path={}'.format(self.dbschema)})
        return engine

    def list_tables(self, ):
        return self.engine.table_names()
    

    def to_dataframe(self, table_name):
        df = pd.read_sql_table(table_name, self.engine)
        return df
    
	def to_geodataframe_by_query(self, query_str, geom_col):
        gdf = gpd.read_postgis(sql=query_str, con=self.engine, geom_col=geom_col)
		return gdf
    
    def to_geodataframe(self, table_name, geom_col):
        sql_str = "select * from {}".format(table_name)
        gdf = self.to_geodataframe_by_query(self, sql_str, geom_col)
        return gdf
    
    def write_dataframe(self, df, table_name):
        df.to_sql(table_name, self.engine)


    def write_geodataframe(self, gdf, table_name, if_exists='replace', geometry_str='geometry'):
        '''
            gdf: geopandas geodataframe
            table_name: 
            if_exists: {â€˜failâ€™, â€˜replaceâ€™, â€˜appendâ€™}
            Geometry: See :class:`geoalchemy2.types._GISType` for the list of arguments that can
    be passed to the constructor
        '''
        gdf.to_sql(name=table_name,
                   con=self.engine,
                   if_exists=if_exists,
                   index=False,
                   dtype={geometry_str: Geometry('POINT', srid=4326)})
  
        



if __name__ == "__main__":

    # ----------------------- å®šä¹‰æ•°æ®åº“å‚æ•° -------------------------#
    # follows django database settings format, replace with your own settings
    DATABASES = {
        'db1': {
            'NAME': 'postgis',
            'USER': 'postgres',
            'PASSWORD': '123345678',
            'HOST': 'localhost',
            'PORT': 5432,
        },
    }

    # choose the database to use
    db = DATABASES['db1']

    # construct an engine connection string
    engine_string = "postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}".format(
        user=db['USER'],
        password=db['PASSWORD'],
        host=db['HOST'],
        port=db['PORT'],
        database=db['NAME'],
    )

    # é€‰æ‹©ç‰¹å®šschemaä¿å­˜ï¼ˆé»˜è®¤ä¿å­˜åœ¨publicï¼‰;publicä¸€å®šè¦åŠ åœ¨å°¾éƒ¨ï¼ˆä¸ç„¶geometryå†™å…¥æ—¶ä¼šæŠ¥é”™ï¼‰ï¼Œé€—å·ä¸èƒ½æœ‰ç©ºæ ¼
    dbschema = 'chongqing,public' 
    # ----------------------- å®šä¹‰æ•°æ®åº“å‚æ•° -------------------------#

    # ----------------------- åˆ—å‡ºè¡¨ -------------------------#
    transit = Transit(engine_string, dbschema)
    transit.list_tables()
    # []
    # ----------------------- åˆ—å‡ºè¡¨ -------------------------#

    # ----------------------- å†™å…¥æ•°æ®è‡³postgresql -------------------------#
    gdf = gpd.read_file('./é‡åº†å¸‚.geojson')
    gdf['wgs_geometry'] = gdf['geometry'].apply(lambda x: WKTElement(x.wkt, srid=4326))
    gdf.drop(['geometry'], axis=1, inplace=True)
    transit.write_geodataframe(gdf, table_name, 'append', geometry_str='wgs_geometry')
    
    #------
    transit.write_dataframe(gdf[['adcode', 'name']], 'chongqing_attr')
    # ----------------------- å†™å…¥æ•°æ®è‡³postgresql -------------------------#

    # ----------------------- ä»postgresqlè¯»å‡ºæ•°æ® -------------------------#
    table_name, geom_col = 'chongqing', 'geometry'
    gdf = transit.to_geodataframe(table_name,geom_col)
    gdf.plot()
    table_name = 'chongqing_attr'
    df = transit.to_dataframe(table_name)
    # ----------------------- ä»postgresqlè¯»å‡ºæ•°æ® -------------------------#
```



#### mongodbäº¤äº’
å°† DataFrame ä¿å­˜è‡³ mongodb
```Python
mongo.collection.insert(json.loads(df.T.to_json()).values())
```

#### to_sqlite
æœ‰æ—¶å€™å¤§æ‰¹é‡çš„df.queryæŸ¥è¯¢å¤ªè€—æ—¶é—´äº†ï¼Œæ²¡æœ‰**sqlæŸ¥è¯¢é€Ÿåº¦å¿«**ã€‚å› æ­¤æƒ³åˆ°çš„ä¸€ä¸ªè§£å†³æ–¹æ¡ˆæ˜¯æŠŠæŸ¥è¯¢ç›®æ ‡çš„DataFrameå­˜å‚¨åˆ°sqliteæ•°æ®åº“ï¼Œç„¶åä½¿ç”¨sqlè¿›è¡ŒæŸ¥è¯¢
```Python
# pip install sqlalchemy
from sqlalchemy import create_engine
import os
import geopandas as gpd
import pandas
from tqdm import tqdm


data_root = "../data"
nodes_fp = os.path.join(data_root, "wh/wh.shp/nodes.shp")
edges_fp = os.path.join(data_root, "wh/wh.shp/edges.shp")
edges_gdf = gpd.read_file(edges_fp)

# Create an in-memory SQLite database.
engine = create_engine('sqlite://', echo=False)
edges_gdf[['fid', 'from', 'to']].to_sql('edges', con=engine, if_exists='replace')

# æŸ¥è¯¢edgesä¸­fidä¸º1ï¼Œ2çš„å…ƒç´ ï¼Œå¹¶å–å€¼fromå’Œto
results = engine.execute("SELECT e.'from', e.'to'  FROM edges as e where e.'fid'in (1,2,3, '')").fetchall()
# results examle: 
# [(267620078, 3488417963), (3488417935, 267620078), (3684965660, 267620078)]

# list è½¬ tupleåæŸ¥è¯¢
_list = [1,2,3]
engine.execute("SELECT e.'from', e.'to'  FROM edges as e where e.'fid'in {}".format(tuple(_list))).fetchall()
```
{{< notice success >}} 
- [df.to_sql](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html); 
- create_engine:å¦‚æœæƒ³ä¿å­˜åˆ°æœ¬åœ°ï¼Œ`sqlite://`åå¡«å…¥åœ°å€å³å¯ï¼Œå¦‚Windowsè·¯å¾„ï¼š`engine = create_engine('sqlite:///C:\\sqlitedbs\\pois.db', echo=True)`åŠlinuxè·¯å¾„ï¼š`engine = create_engine('sqlite:////workspace/UrbanFunctionalRegionalization/20210702-wh/data/poi/pois.sqlite', echo=True)`
- æ­¤å¤„è½¬æ¢DataFrameä¸ºä¸€ä¸ªSQLiteæ•°æ®åº“ï¼Œæ”¾åœ¨å†…å­˜ä¸­
- if_exists : {'fail', 'replace', 'append'}, default 'fail'
  * fail: Raise a ValueError.
  * replace: Drop the table before inserting new values.
  * append: Insert new values to the existing table.
- to_sqlè¿˜å¯ä»¥é€šè¿‡`dtype={"A": Integer()}`æ¥å®šä¹‰æ•°æ®ç±»å‹ï¼Œéœ€å…ˆå¼•å…¥`from sqlalchemy.types import Integer`; sqlalchemyé€šç”¨æ•°æ®ç±»å‹ğŸ’¨[sqlalchemy.types](https://docs.sqlalchemy.org/en/14/core/type_basics.html#generic-types)
{{< /notice >}}


### æ•°æ®å¤„ç†

#### åŸºç¡€æ“ä½œ

##### æ•°æ®ç­›é€‰ï¼ˆè¡Œæ“ä½œï¼‰
åœ¨ç­›é€‰æ•°æ®çš„æ—¶å€™ï¼Œæˆ‘ä»¬ä¸€èˆ¬ç”¨`df[æ¡ä»¶]`çš„æ ¼å¼ï¼Œå…¶ä¸­çš„æ¡ä»¶ï¼Œæ˜¯å¯¹dataæ¯ä¸€è¡Œæ•°æ®çš„trueå’Œfalseå¸ƒå°”å˜é‡çš„Series
- æ¡ä»¶ï¼šä¾‹å¦‚ï¼Œæˆ‘ä»¬æƒ³å¾—åˆ°è½¦ç‰Œç…§ä¸º22271çš„æ‰€æœ‰æ•°æ®ã€‚é¦–å…ˆæˆ‘ä»¬è¦è·å¾—ä¸€ä¸ªå¸ƒå°”å˜é‡çš„Seriesï¼Œè¿™ä¸ªSerieså¯¹åº”çš„æ˜¯dataçš„æ¯ä¸€è¡Œï¼Œå¦‚æœè½¦ç‰Œç…§ä¸º"ç²¤B4H2K8"åˆ™ä¸ºtrueï¼Œä¸æ˜¯åˆ™ä¸ºfalseã€‚è¿™æ ·å­çš„Serieså¾ˆå®¹æ˜“è·å¾—ï¼Œåªéœ€è¦`df['VehicleNum']==22271`
- **ç­›é€‰**æ•°æ®ï¼š
  - å•ä¸€æ¡ä»¶`df[df['VehicleNum']==22271]`
  - å¤šæ¡ä»¶ï¼š
    - å¹¶ï¼š`df[(df['popularity'] > 3) & (df['popularity'] < 7)]`
    - æˆ–ï¼š`df[(df['popularity'] < 3) | (df['popularity'] > 7)]`
  - è¿”å›æ»¡è¶³æ¡ä»¶çš„è¡Œå·(ç´¢å¼•)ï¼š`np.where(df['VehicleNum']==22271)`
  - **æå–æŸä¸€è¡Œæ•°æ®**ï¼š`df.iloc[32]`
  - æå–popularityåˆ—æœ€å¤§å€¼æ‰€åœ¨è¡Œ: `df[df['popularity'] == df['popularity'].max()]`
- åå‘ç­›é€‰ï¼š`data[-(æ¡ä»¶)]`ï¼Œä¾‹å¦‚: `data[-(data['VehicleNum']==22271)]`
- **æ·»åŠ ä¸€è¡Œ**æ•°æ®: `df = df.append({'grammer':'Perl','popularity':6.6},ignore_index=True)`
- **å»é™¤é‡å¤**è¡Œï¼š `df.drop_duplicates(subset=None, keep='first', inplace=False)`
	- subset : column label or sequence of labels, optional ç”¨æ¥æŒ‡å®šç‰¹å®šçš„åˆ—ï¼Œé»˜è®¤æ‰€æœ‰åˆ—
	- keep : {â€˜firstâ€™, â€˜lastâ€™, False}, default â€˜firstâ€™ åˆ é™¤é‡å¤é¡¹å¹¶ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„é¡¹
	- inplace : boolean, default False æ˜¯ç›´æ¥åœ¨åŸæ¥æ•°æ®ä¸Šä¿®æ”¹è¿˜æ˜¯ä¿ç•™ä¸€ä¸ªå‰¯æœ¬
	- å‚è€ƒ: [drop_duplicates](https://blog.csdn.net/u010665216/article/details/78559091)
- å°†**æ•°æ®æ’åº**,å¹¶æŠŠæ’åºåçš„æ•°æ®èµ‹å€¼ç»™åŸæ¥çš„æ•°æ®ï¼š
```Python
df = df.sort_values(by = ['VehicleNum','Stime'], ascending = True)
#ascending: True å‡åº,False é™åº
```
- **éå†**è¡Œ: å¦‚æœå¿…é¡»è¦è¦ç”¨iterrowsï¼Œå¯ä»¥ç”¨itertuplesæ¥è¿›è¡Œæ›¿æ¢ã€‚**åœ¨ä»»ä½•æƒ…å†µä¸‹itertupleséƒ½æ¯”iterrowså¿«å¾ˆå¤šå€**ã€‚
  ```python
  for row in df.itertuples():
      print(getattr(row, 'c1'), getattr(row, 'c2'))
  ```

##### è·å–/åˆ é™¤/å®šä¹‰DataFrameçš„æŸä¸€åˆ—ï¼ˆåˆ—æ“ä½œï¼‰
- è·å–åˆ—'Stime'ï¼š`df['Stime']`æˆ–`df.loc[:,'Stime']`
- åˆ é™¤åˆ—'Stime'ï¼š`df.drop(['Stime'],axis=1)`
- è·å–æŸä¸€åˆ—æŸä¸€è¡Œçš„æ•°æ®ï¼š`df['Stime'].iloc[3] #è·å–Stimeåˆ—çš„ç¬¬4è¡Œæ•°æ®`
- åˆ—ï¼ˆColumnsï¼‰**é‡å‘½å**ï¼š`df.rename(columns={"x": "pu_x", "y": "pu_y"}, inplace=True)`
- æŸä¸€åˆ—**ç±»å‹è½¬æ¢**ï¼š`df['salary'].astype(np.float64)`
- ç´¢å¼•ï¼š
- **é‡ç½®è¡Œå·**ï¼š`df.reset_index()`
- è®¾ç½®ç´¢å¼•ï¼š`df.set_index('car_id')`
- **ç»Ÿè®¡å‡ºç°é¢‘ç‡/æ¬¡æ•°**ï¼šä¾‹å¦‚ï¼Œ
- æŸ¥çœ‹æ¯ç§å­¦å†å‡ºç°çš„æ¬¡æ•°ï¼š`df.education.value_counts()`
- æŸ¥çœ‹educationåˆ—å…±æœ‰å‡ ç§å­¦å†ï¼š`df.education.nunique()`

##### æŸ¥çœ‹DataFrameåŸºæœ¬ä¿¡æ¯
- æŸ¥çœ‹ç´¢å¼•ã€æ•°æ®ç±»å‹å’Œå†…å­˜ä¿¡æ¯ï¼š`df.info()`
- æŸ¥çœ‹æ•°å€¼å‹åˆ—çš„æ±‡æ€»ç»Ÿè®¡ï¼š `df.describe()`
- æŸ¥çœ‹dfæ‰€æœ‰æ•°æ®çš„æœ€å°å€¼ã€25%åˆ†ä½æ•°ã€ä¸­ä½æ•°ã€75%åˆ†ä½æ•°ã€æœ€å¤§å€¼ï¼š`np.percentile(df, q=[0, 25, 50, 75, 100])`
- **EDAåˆ†æ**(æ•°æ®å¯è§†åŒ–): [sweetviz](https://towardsdatascience.com/sweetviz-automated-eda-in-python-a97e4cabacde)![sweetvizå¿«é€ŸEDAç¤ºä¾‹](https://cdn.jsdelivr.net/gh/xunhs/image_host/history/uploads/images/2020/S3/20200711175504.png)
	- Init:
```python
  import sweetviz as sz
  import pandas as pd
  df = pd.read_csv('train_set.csv', header=0)
  df1 = pd.read_csv('test_set.csv', header=0)
```
	- ç»¼åˆæŠ¥å‘Š:å¸¸è§æ•°æ®ç‰¹å¾æŠ¥å‘Šï¼Œ [link](https://cdn.jsdelivr.net/gh/xunhs/image_host/assets/python/sweetviz/Advertising.html)
	```python
	advert_report = sz.analyze(df)
	advert_report.show_html('Advertising.html')
	```
	
	- å¯¹æ¯”æŠ¥å‘Š:å¦‚è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¯¹æ¯”ï¼Œ [link](https://cdn.jsdelivr.net/gh/xunhs/image_host/assets/python/sweetviz/Comparing.html)
	```python
	compare_report = sz.compare(df.drop('y', axis=1), df1)
	compare_report.show_html('Comparing.html')
	```


- **PandasåŸºæœ¬æ•°æ®ç±»å‹**dtype![](https://gitee.com/xunhs/xunhs/raw/master/pics/2020/spring/20200312175149.png)[å‚è€ƒ](https://pbpython.com/pandas_dtypes.html)


##### ç¼ºå¤±å€¼
- æŸ¥çœ‹æ¯åˆ—æ•°æ®ç¼ºå¤±å€¼æƒ…å†µï¼š`df.isnull().sum()`
- æå–æ—¥æœŸåˆ—**å«æœ‰ç©ºå€¼çš„è¡Œ**ï¼š`df[df.datetime.isnull()]`
- åˆ é™¤å­˜åœ¨ç¼ºå¤±å€¼çš„è¡Œï¼š`df.dropna(axis=0, how='any', inplace=True)`
  - axisï¼š0-è¡Œæ“ä½œï¼ˆé»˜è®¤ï¼‰ï¼Œ1-åˆ—æ“ä½œ
  - howï¼šany-åªè¦æœ‰ç©ºå€¼å°±åˆ é™¤ï¼ˆé»˜è®¤ï¼‰ï¼Œall-å…¨éƒ¨ä¸ºç©ºå€¼æ‰åˆ é™¤


##### å…³è”å’Œåˆå¹¶
- åˆå¹¶concatï¼ˆè½´å‘è¿æ¥ï¼‰ï¼ˆæ— éœ€é”®å€¼ï¼Œç›´æ¥åˆå¹¶ï¼ŒAå’ŒBå…·æœ‰ç›¸åŒçš„ç»“æ„ï¼‰
```python
# pd.concat([A, B]) # æœ‰[]
pd.concat([A, B], axis=1) # åˆ—ä¹‹é—´æ‹¼æ¥
pd.concat([A, B], axis=0) # è¡Œä¹‹é—´æ‹¼æ¥
```
- å…³è”mergeï¼ˆ[æ•°æ®åº“é£æ ¼çš„åˆå¹¶](https://blog.csdn.net/weixin_38168620/article/details/80663892)ï¼‰ï¼ˆéœ€æŒ‡å®šé”®å€¼ï¼Œä¾ç…§é”®å€¼åŒ¹é…å…³ç³»è¿æ¥ï¼‰
```python
# pd.merge(A, B, left_on, right_on, how) # æ— []
pd.merge(A, B, left_on='airport_ref', right_on='id', how='inner')
```

#### query()
å‚è€ƒ[åŸºäºquery()çš„é«˜æ•ˆæŸ¥è¯¢](https://www.cnblogs.com/feffery/p/13440148.html)

##### ç¤ºä¾‹
æ‰¾å‡ºç±»å‹ä¸ºTV Showä¸”å›½å®¶ä¸å«ç¾å›½çš„Kids' TV
![](https://cdn.jsdelivr.net/gh/xunhs/image_host/images/2020/8/1597799020.png)

##### å¸¸ç”¨ç‰¹æ€§
- ç›´æ¥è§£æå­—æ®µå
åœ¨ä½¿ç”¨query()æ—¶æˆ‘ä»¬åœ¨ä¸éœ€è¦é‡å¤ä¹¦å†™æ•°æ®æ¡†åç§°[å­—æ®µå]è¿™æ ·çš„å†…å®¹ï¼Œå­—æ®µåä¹Ÿç›´æ¥å¯ä»¥å½“ä½œå˜é‡ä½¿ç”¨ï¼Œè€Œä¸”ä¸åŒæ¡ä»¶ä¹‹é—´ä¸éœ€è¦ç”¨æ‹¬å·éš”å¼€ï¼Œåœ¨æ¡ä»¶ç¹æ‚çš„æ—¶å€™ç®€åŒ–ä»£ç çš„æ•ˆæœæ›´ä¸ºæ˜æ˜¾ã€‚
- é“¾å¼è¡¨è¾¾å¼
  ```python
  demo = pd.DataFrame({
      'a': [5, 4, 3, 2, 1],
      'b': [1, 2, 3, 4, 5]
  })
  
  demo.query("a <= b != 4")
  ```
- æ”¯æŒinä¸not inåˆ¤æ–­: `netflix.query("release_year in [2018, 2019]")`
- å¯¹å¤–éƒ¨å˜é‡çš„æ”¯æŒ:query()è¡¨è¾¾å¼è¿˜æ”¯æŒä½¿ç”¨å¤–éƒ¨å˜é‡ï¼Œåªéœ€è¦åœ¨å¤–éƒ¨å˜é‡å‰åŠ ä¸Š@ç¬¦å·å³å¯
  ```python
  years = [2018, 2019]
  netflix.query("release_year in @years")
  ```
- å¯¹å¸¸è§„è¯­å¥çš„æ”¯æŒ: å¯ä»¥ç›´æ¥è§£æPythonè¯­å¥ï¼Œæå¤§åœ°è‡ªç”±åº¦
  ```python
  def country_count(s):
      '''
      è®¡ç®—æ¶‰åŠå›½å®¶æ•°é‡
      '''
      return s.split(',').__len__()
  
  # æ‰¾å‡ºå‘è¡Œå¹´ä»½åœ¨2018æˆ–2019å¹´ä¸”åˆä½œå›½å®¶æ•°é‡è¶…è¿‡5ä¸ªçš„å‰§é›†
  netflix.query("release_year.isin([2018, 2019]) and country.apply(@country_count) > 5")
  ```
- å¯¹Indexä¸MultiIndexçš„æ”¯æŒ

#### apply()

##### apply + lambda
```Python
data.gender.apply(lambda x:'å¥³æ€§' if x is 'F' else 'ç”·æ€§')
# ç­‰åŒäº: data.gender.map({'F': 'å¥³æ€§', 'M': 'ç”·æ€§'})
```

##### applyè¾“å…¥å¤šå‚
```Python
def _get_coordinates(row, points_df):
    return points_df[points_df.point_id.isin(row.traj_points)]
	.apply(lambda row: (row.x, row.y), axis=1).tolist()

trajs_df['coordinates'] = trajs_df.progress_apply(_get_coordinates, axis=1, args=(points_df,))
```
- ä½¿ç”¨argsè¾“å…¥å¤šå‚æ•°
- å‡½æ•°å‚æ•°åˆ—è¡¨ä¸­ï¼Œ**rowæ”¾åœ¨ç¬¬ä¸€ä¸ªï¼Œå…¶ä»–å‚æ•°å‘åå»¶ç»­**

##### apply è¾“å…¥å¤šåˆ—æ•°æ®
```Python
def generate_descriptive_statement(year, name, gender, count):
    year, count = str(year), str(count)
    gender = 'å¥³æ€§' if gender is 'F' else 'ç”·æ€§'
    return 'åœ¨{}å¹´ï¼Œå«åš{}æ€§åˆ«ä¸º{}çš„æ–°ç”Ÿå„¿æœ‰{}ä¸ªã€‚'.format(year, name, gender, count)

data.apply(lambda row:generate_descriptive_statement(row['year'],
                                                      row['name'],
                                                      row['gender'],
                                                      row['count']),
           axis = 1)
```
- **axis=1** å¤„ç†å¤šä¸ªå€¼æ—¶è¦ç»™apply()æ·»åŠ å‚æ•°axis=1
- `row['year'], row['gender']` ç›´æ¥ç”¨åˆ—åå³å¯ï¼ˆ`row.year, row.gender`ä¹Ÿæ˜¯å¯ä»¥çš„ï¼‰


##### apply è¾“å‡ºå¤šåˆ—æ•°æ®
```Python
# æå–nameåˆ—ä¸­çš„é¦–å­—æ¯å’Œå‰©ä½™éƒ¨åˆ†å­—æ¯
_apply = data.apply(lambda row: (row['name'][0], row['name'][1:]), axis=1)
a, b = zip(*list(_apply))
```
- zip(*zipped)æ¥è§£å¼€å…ƒç»„åºåˆ—;åŒæ ·åœ¨å‡½æ•°ä¼ å‚çš„è¿‡ç¨‹ä¸­ï¼Œ`**args`ä¹Ÿå¯ä»¥è§£å¼€argså­—å…¸å˜æ¢å‚æ•°å½¢å¼ã€‚



##### apply + [swifterå¹¶è¡Œ](https://github.com/jmcarpenter2/swifter/blob/master/docs/documentation.md)

> 2020.9.5 Note: swifter.applyåŠ é€Ÿæ•ˆæœå¾ˆæ˜æ˜¾ï¼›è¯»å–å¤§æ–‡ä»¶å¯ä»¥ä½¿ç”¨modin.pandasè¿›è¡Œè¯»å–ï¼Œapplyç­‰æ“ä½œå¯ä»¥ä½¿ç”¨swifterè¿›è¡ŒåŠ é€Ÿã€‚;å¦swifter.applyçš„å‡½æ•°ä¸­ä¸å¯å®šä¹‰[vectorized form](https://github.com/jmcarpenter2/swifter/blob/master/examples/swifter_apply_examples.ipynb)ï¼ˆå¦‚ifå‡½æ•°ï¼‰ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´åŠ é€Ÿæ•ˆæœä¸æ˜æ˜¾ã€‚

```Python
import pandas as pd
import swifter

df = pd.DataFrame({'x': [1, 2, 3, 4], 'y': [5, 6, 7, 8]})

# runs on single core
df['x2'] = df['x'].apply(lambda x: x**2)
# runs on multiple cores
df['x2'] = df['x'].swifter.apply(lambda x: x**2)

# use swifter apply on whole dataframe
df['agg'] = df.swifter.apply(lambda x: x.sum() - x.min())

# use swifter apply on specific columns
df['outCol'] = df[['inCol1', 'inCol2']].swifter.apply(my_func)
```


#### æ—¶é—´å¤„ç†

![](https://gitee.com/xunhs/xunhs/raw/master/pics/2020/summer/20200601153141.png)
å‚è€ƒ: [pandas.pydata](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#)
å¦è§[datetimeæ—¶é—´å¤„ç†](https://xunhs.press/2020/04/5cdaf520/#%E6%97%B6%E9%97%B4%E5%A4%84%E7%90%86)


##### parse_dates
åœ¨ `read_csv()` æ–¹æ³•ä¸­ï¼Œé€šè¿‡ parse_dates å‚æ•°ç›´æ¥å°†æŸäº›åˆ—è½¬æ¢æˆ datetime64 ç±»å‹, index_colè®¾ç½®ç´¢å¼•
```python
df1 = pd.read_csv('sample-salesv3.csv', parse_dates=['date'], index_col='date')
```

##### to_datetime

###### Timestamp(æ—¶é—´ç‚¹)
```python
# unix time2datetime
pd.to_datetime(1490195805, unit='s')
# => Timestamp('2017-03-22 15:16:45')

# datetime str2Timestamp
pd.to_datetime("2017-11-01 12:24")
# or setting format
pd.to_datetime("2017å¹´11æœˆ1æ—¥ 12æ—¶24åˆ†", format='%Yå¹´%mæœˆ%dæ—¥ %Hæ—¶%Måˆ†')
#=> Timestamp('2017-11-01 12:24:00')
```
- (Attributes)[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html]:  
	- å¹´æœˆæ—¥ï¼ˆyear, month, dayï¼‰
	- æ™‚åˆ†ç§’ï¼ˆhour, minute, secondï¼‰ 

###### DatetimeIndex(æ—¶é—´åºåˆ—ç´¢å¼•)
```python
pd.to_datetime([1490195805.433, 1490195805.433502912], unit='s')
#=>DatetimeIndex(['2017-03-22 15:16:45.433000088', '2017-03-22 15:16:45.433502913'], dtype='datetime64[ns]', freq=None)
```
- (Attributes)[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html]:  
	- å¹´æœˆæ—¥ï¼ˆyear, month, dayï¼‰, 
	- æ™‚åˆ†ç§’ï¼ˆhour, minute, secondï¼‰
- unix timeå½¢å¼
```python
pd.to_datetime(['2017-03-22 15:16:45.433000088', '2017-03-22 15:16:45.433502913']).astype(int) / 10**9
#=> Float64Index([1490195805.433, 1490195805.433503], dtype='float64')
```
å‚è€ƒ: https://stackoverflow.com/questions/54313463/pandas-datetime-to-unix-timestamp-seconds

##### date_range
Return a fixed frequency DatetimeIndex. å‚è€ƒ: [pandas.date_range](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html)
###### å¸¸ç”¨å‚æ•°:
- start: Left bound for generating dates.
- end: Right bound for generating dates.
- periods: Number of periods to generate.
- freq: Frequency strings can have multiples. å‚è€ƒ: [timeseries-offset-aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases)
  - D: calendar day frequency
  - M: month end frequency
  - Y: year end frequency
  - H: hourly frequency
  - T: minutely frequency
  - S: secondly frequency
  - Q: å­£åº¦

###### examples:
```python
pd.date_range(start='1/1/2018', end='1/08/2018')
#=> DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04','2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],dtype='datetime64[ns]', freq='D')

# å¼€å§‹ä¸º2018.1.1, å–8ä¸ªæ—¥æœŸï¼Œé»˜è®¤é—´éš”ä¸ºå¤©
pd.date_range(start='1/1/2018', periods=8)
#=> DatetimeIndex(['2018-01-01', '2018-01-02', '2018-01-03', '2018-01-04','2018-01-05', '2018-01-06', '2018-01-07', '2018-01-08'],dtype='datetime64[ns]', freq='D')

# ä¸‰ä¸ªæœˆä¸ºé—´éš”
pd.date_range(start='1/1/2018', periods=5, freq='3M')
#=> DatetimeIndex(['2018-01-31', '2018-04-30', '2018-07-31', '2018-10-31', '2019-01-31'], dtype='datetime64[ns]', freq='3M')

```

##### æ—¥æœŸæ£€ç´¢
```python
test_df = pd.DataFrame({'data': range(1, 1000)})
test_df.index = pd.date_range(start='2020-1-1', end='2020-6-1', periods=test_df.shape[0])

# è·å–2020å¹´çš„æ•°æ®
test_df['2020']
# è·å–2020å¹´5æœˆçš„æ•°æ®
test_df['2020-5']
# è·å–2020å¹´5æœˆ1å·çš„æ•°æ®
test_df['2020-5-1']
# è·å–2020å¹´ä¸€å­£åº¦(1,2,3æœˆ)çš„æ•°æ®
test_df['2020Q1']
# è·å–2020å¹´5æœˆ1å·åˆ°2020å¹´5æœˆ30å·çš„æ•°æ®
test_df['2020-5-1':'2020-5-30']

```


#### èšåˆç±»æ–¹æ³• groupby() + agg() 
å‚è€ƒï¼šhttps://www.cnblogs.com/feffery/p/11468762.html
è¦è¿›è¡Œåˆ†ç»„è¿ç®—ç¬¬ä¸€æ­¥å½“ç„¶å°±æ˜¯åˆ†ç»„ï¼Œåœ¨pandasä¸­å¯¹æ•°æ®æ¡†è¿›è¡Œåˆ†ç»„ä½¿ç”¨åˆ°groupby()æ–¹æ³•ï¼Œå…¶ä¸»è¦ä½¿ç”¨åˆ°çš„å‚æ•°ä¸ºbyï¼Œè¿™ä¸ªå‚æ•°ç”¨äºä¼ å…¥åˆ†ç»„ä¾æ®çš„å˜é‡åç§°ï¼Œ**å½“å˜é‡ä¸º1ä¸ªæ—¶ä¼ å…¥åç§°å­—ç¬¦ä¸²å³å¯ï¼Œå½“ä¸ºå¤šä¸ªæ—¶ä¼ å…¥è¿™äº›å˜é‡åç§°åˆ—è¡¨**ï¼ŒDataFrameå¯¹è±¡é€šè¿‡groupby()ä¹‹å**è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨**ï¼Œéœ€è¦å°†å…¶åˆ—è¡¨åŒ–æ‰èƒ½å¾—åˆ°éœ€è¦çš„åˆ†ç»„åçš„å­é›†
```Python
group_df = trajs_with_id_df.groupby(by=['car_id'])[['traj_id', 'traj_points']]
groups = [group for group in group_df]
groups[0]
```
- output:
![](https://gitee.com/xunhs/xunhs/raw/master/pics/2020/spring/20200228082637.png)
- æ¯ä¸€ä¸ªç»“æœéƒ½æ˜¯ä¸€ä¸ªäºŒå…ƒç»„ï¼Œ**å…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¯¹åº”è¿™ä¸ªåˆ†ç»„ç»“æœçš„åˆ†ç»„ç»„åˆæ–¹å¼ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯åˆ†ç»„å‡ºçš„å­é›†æ•°æ®æ¡†**
```Python
groups = data_df.groupby(by=['assigned_c', 'osmid'])[['x', 'y']].max().reset_index(drop=False)
```
- by=['assigned_c', 'osmid'],åˆ†ç»„çš„ç»„åˆæ–¹å¼
- [['x', 'y']]åˆ†ç»„åå–å‡ºxå’Œyåˆ—è¿›è¡Œåç»­æ“ä½œ
- max() æ ¹æ®åˆ†ç»„å¯¹xå’Œyåˆ—å–æœ€å¤§å€¼
- reset_index(drop=False) é‡ç½®ç´¢å¼•ï¼Œä¾¿äºæ˜¾ç¤ºå’Œå–å€¼; drop: æ˜¯å¦ä¸¢æ‰åŸç´¢å¼•
```Python
agg_df = data_df.groupby(by=['assigned_c']).agg({'x': ['mean', 'max', 'min', 'mean', 'std']})
										.reset_index(drop=False)

```
- agg() å³aggregateï¼Œèšåˆï¼›ä¼ å…¥å­—å…¸ï¼šæ“ä½œåˆ—ä¸ºkeyå’Œç›¸å…³æ“ä½œä¸ºvalue

```Python
agg_df = data_df.groupby(by=['assigned_c']).agg(mean_x=pd.NamedAgg(column='x', aggfunc='mean'),
                                        max_x=pd.NamedAgg(column='x', aggfunc='max'),
                                        min_x=pd.NamedAgg(column='x', aggfunc='min'),)
										.reset_index(drop=False)

```
- ä½¿ç”¨pd.NamedAgg()æ¥ä¸ºèšåˆåçš„æ¯ä¸€åˆ—èµ‹äºˆæ–°çš„åå­—


### å¼‚å¸¸è°ƒè¯•

#### å¿½ç•¥warningå…¨å±€è®¾ç½®
```Python
import warnings
warnings.filterwarnings('ignore')
```

#### ç¼–ç é”™è¯¯ï¼ˆillegal multibyte sequenceï¼‰
```Python
def get_df(file_path): 
    poi_df = None
    with open(file_path, encoding='gb2312', errors='ignore') as fp:
        poi_df = pd.read_csv(fp, header=0)
        return poi_df
```

#### æ˜¾ç¤ºè®¾ç½®
- å–æ¶ˆç§‘å­¦è®¡æ•°æ³•æ˜¾ç¤º
å‚è€ƒï¼šhttps://blog.csdn.net/chenpe32cp/article/details/87883420
ç§‘å­¦è®¡æ•°æ³•çš„æ˜¾ç¤ºå¾ˆéš¾é˜…è¯»ã€‚å–æ¶ˆç§‘å­¦è®¡æ•°æ³•æ˜¾ç¤ºï¼Œä¿ç•™å°æ•°åä¸¤ä½ã€‚
```python
# å…¨å±€è®¾ç½®
pd.set_option('display.float_format',lambda x : '%.2f' % x)
# æˆ–
# å•ä¸ªDataFrameç”Ÿæ•ˆ
df = pd.DataFrame(np.random.random(10)**10, columns=['data'])
df.round(3)
```

- æ˜¾ç¤ºæ‰€æœ‰åˆ—æˆ–æ‰€æœ‰è¡Œ
å‚è€ƒï¼šhttps://blog.csdn.net/qq_42648305/article/details/89640714
```python
pd.options.display.max_columns = None
pd.options.display.max_rows = None
```


### Geopandas

#### I/O
å‚è€ƒï¼šhttps://geopandas.org/io.html#writing-spatial-data 
- æ”¯æŒçš„æ ¼å¼ï¼š
```python
import fiona
fiona.supported_drivers 

{'AeronavFAA': 'r',
 'ARCGEN': 'r',
 'BNA': 'rw',
 'DXF': 'rw',
 'CSV': 'raw',
 'OpenFileGDB': 'r',
 'ESRIJSON': 'r',
 'ESRI Shapefile': 'raw',
 'FlatGeobuf': 'rw',
 'GeoJSON': 'raw',
 'GeoJSONSeq': 'rw',
 'GPKG': 'raw',
 'GML': 'rw',
 'OGR_GMT': 'rw',
 'GPX': 'rw',
 'GPSTrackMaker': 'rw',
 'Idrisi': 'r',
 'MapInfo File': 'raw',
 'DGN': 'raw',
 'OGR_PDS': 'r',
 'S57': 'r',
 'SEGY': 'r',
 'SUA': 'r',
 'TopoJSON': 'r'}

```
- Read Data
  - read_file
  ```python
  geopandas.read_file(fp)
  # gdb or gpkg's layer
  gdb_fp = '/workspace/UrbanFunctionalRegionalization/map_doc/regionalization.gdb'
  thiessen_gdf = gpd.geopandas.read_file(gdb_fp, layer='thiessen_cliped')
  ```
- Write Data
  For a full list of supported formats, type `import fiona; fiona.supported_drivers`
  - Writing to Shapefile
  ```python
  countries_gdf.to_file("countries.shp")
  ```
  - Writing to GeoJSON file
  ```python
  countries_gdf.to_file("countries.geojson", driver='GeoJSON')
  ```
  - Writing to GeoJSON string
  ```python
  countries_gdf.to_json()
  ```
- Writing to GeoPackage
  ```python
  countries_gdf.to_file("package.gpkg", layer='countries', driver="GPKG")
  cities_gdf.to_file("package.gpkg", layer='cities', driver="GPKG")
  ```

#### DataFrame2GeoDataFrame

##### set_geometry
- set_geometry
```python
trajs_gpd_df = trajs_df.set_geometry('line_geo')
```
- åœ¨åˆå§‹åŒ–GeoDataFrameæ—¶å®šä¹‰geometry
```python
from shapely.geometry import Point, LineString
geometry = [Point(), Point(), ...] or [LineString([p1, p2, ...]), ...] or ...
gpd.GeoDataFrame(df, geometry=[])
```

##### ç©ºé—´è¿æ¥/Spatial Join
å‚è€ƒï¼šhttps://blog.csdn.net/qq_28360131/article/details/81165168
shaplyåŒ…çš„vectorizedåŒ…å«ç€ä¸€äº›å¯¹æŸ¥è¯¢çš„ä¼˜åŒ–ï¼Œé€šè¿‡shaplyå’Œgeopandasä¸€èµ·åä½œå¯ä»¥è¾¾åˆ°è¾ƒå¥½çš„ä¼˜åŒ–æ•ˆæœã€‚

```python
import shapely.vectorized as sv
point_df = point_df[['id', 'lon', 'lat']]

for row in ploy_gdf.itertuples():
    geometry, area_id = getattr(row, "geometry"), getattr(row, "area_id")
    point_df.loc[sv.contains(geometry, x=point_df.lon, y=point_df.lat), "AreaID"] = area_id

```
- point_df

|      |  id  |   lon   |   lat   |      datetime       | area_id |
| :--: | :--: | :-----: | :-----: | :-----------------: | :-----: |
|  0   |  0   | 116.41  | 39.9084 | 2012-11-02 00:25:03 |   94    |
|  1   |  1   | 116.583 | 40.0793 | 2012-11-02 01:25:41 |   nan   |
|  2   |  2   | 116.34  | 39.9567 | 2012-11-02 02:06:14 |   145   |
|  3   |  3   | 116.343 | 39.9126 | 2012-11-02 02:19:51 |   86    |
|  4   |  4   | 116.334 | 39.906  | 2012-11-02 02:26:55 |   82    |



***

![](https://gitee.com/xunhs/xunhs/raw/master/pics/2020/spring/20200307175915.jpg)
