---
layout: post
title: 转载-Chinese NLP
top: false
categories:
  - 收藏
tags:
  - 转载
  - NLP
  - Chinese NLP
abbrlink: 2e441d2b
date: 2020-08-10T09:29:56+00:00
---

> 转载自[Chinese NLP](https://chinesenlp.xyz)，根据个人理解做适当札记。


<!--more-->

# Chinese Word Segmentation
## Overview

Chinese is written using characters (hanzi), where each character represents a syllable. A word is usually taken to consist of one or more character tokens.  There are no spaces between words. Less than 3500 distinct characters are normally encountered. Word segmentation (or tokenization) is the process of dividing up  a sequence of characters into a sequence of words.

## Example

Input:

```
亲 请问有什么可以帮您的吗？
```

Output:

```
亲 请问 有 什么 可以 帮 您 的 吗 ？
```

## Metrics

Word F1 score:

> Gold: 共同  创造  美好  的  新  世纪  ——  二○○一年  新年  贺词

> Hypothesis: 共同  创造  美  好  的  新  世纪  ——  二○○一年  新年  贺词

Precision = 9 / 11 = 0.818

Recall = 9 / 10 = 0.9

F1 = 0.857

# Chinese Word Embeddings
## Background

Word embedding ingests a large corpus of text and outputs, for each word type, an n-dimensional vector of real numbers.  This vector captures syntactic and semantic information about the word that can be employed to solve various NLP tasks.  In Chinese, the unit of encoding may be a character or a sub-character unit, rather than a word.  [词嵌入（Word embedding）摄取了大量的文本语料，并为每个词类型输出一个n维实数向量。 这个向量捕获了关于该词的语法和语义信息，可以用来解决各种NLP任务。 在中文中，编码单位可以是一个字符或一个子字符单位，而不是一个词]

## Example

Input:

```
Large corpus of text
```

Output:
```
“查询”, vec(W) = [-0.059569, 0.126913, 0.273161, 0.225467, -0.185914, 0.018743, -0.18434, 0.083859, -0.115781, -0.216993, 0.063437, -0.005511, 0.276968,…, 0.254486]

```
## Standard Metrics

Word vectors can be evaluated **intrinsically** (e.g., whether similar words get similar vectors) or **extrinsically** (e.g., to what extent word vectors can improve a sentiment analyzer). [词向量可以从内在（例如，相似的词是否得到相似的向量）或外在（例如，词向量可以在多大程度上改进情感分析器）进行评估。]

Intrinsic evaluation looks at
* Word relatedness : Spearman correlation (⍴) between human-labeled scores and scores generated by the embeddings on Chinese word similarity datasets wordsim-240 and wordsim-296 (**translations of English resources**).
* Word Analogy: Accuracy on the word analogy task (e.g: “ 男人 (man) : 女人 (woman) :: 父亲 (father) : X ”, where X chosen by cosine similarity). Different types of word analogy tasks (1) Capitals of countries (2) States/provinces of cities (3) Family words

Extrinsic evaluation:
* Accuracy on Chinese sentiment analysis task
* F1 score on Chinese named entity recognition task
* Accuracy on part-of-speech tagging task

See e.g. [Torregrossa et al., 2020](https://www.aclweb.org/anthology/2020.lrec-1.589.pdf) for a more detailed comparison of metrics

## Word relatedness-Chinese word similarity lists
* wordsim-240 and wordsim-296 list pairs of Chinese words with human similarity judgments proposed by [Chen et. al. (2015)](http://nlp.csai.tsinghua.edu.cn/~lzy/publications/ijcai2015_character.pdf) and [SemEval Task 4: Evaluating Chinese Word Similarity](http://ixa2.si.ehu.es/starsem/proc/pdf/STARSEM-SEMEVAL049.pdf).
* These are Chinese translations of the English lists prepared in 2002.
* Datasets can be found at [https://github.com/Leonard-Xu/CWE/tree/master/data](https://github.com/Leonard-Xu/CWE/tree/master/data)

| Test set | # word pairs with human similarity judgments |
| --- | --- |
| [wordsim-240](https://github.com/Leonard-Xu/CWE/blob/master/data/240.txt) | 240 |
| [wordsim-296](https://github.com/Leonard-Xu/CWE/blob/master/data/297.txt) | 297 |



### Metrics

* Spearman correlation (⍴) between human-labeled scores and scores generated by the embeddings on Chinese word similarity datasets.
* Implementation: [https://github.com/HKUST-KnowComp/JWE/blob/master/src/word_sim.py](https://github.com/HKUST-KnowComp/JWE/blob/master/src/word_sim.py)


### Results

* The SoTA system (VCWE) published in NAACL 2019, combines intra-character compositionality (computed via convolutional neural network ) and inter-character compositionality (computed via a recurrent neural network with self-attention) to compute the word embeddings

| System | wordsim-240 (⍴) | wordsim-296 (⍴) |
| --- | --- | --- |
| [Sun et. al. (2019)](https://arxiv.org/pdf/1902.08795.pdf) (VCWE) | 57.81 | 61.29 |
| [Yu et. al. (2017)](https://www.aclweb.org/anthology/D17-1027) (JWE) | 51.92 | 59.84 |

## Word Analogy-Chinese word analogy lists

Given “France : Paris :: China : ?”, a system should come up with the answer “Beijing”.
* Chen et. al. (2015) manually constructed 1,225 analogies in 3 domains
   * capitals of countries, state/provinces of cities, family relationships
* Datasets can be found at: [https://github.com/Leonard-Xu/CWE/blob/master/data/analogy.txt](https://github.com/Leonard-Xu/CWE/blob/master/data/analogy.txt)

| Test set | # analogies |
| --- | --- |
| Capitals of countries | 687 |
| States/provinces of cities | 175 |
| Family relationships | 240 |

### Metrics

* Accuracy.
* Implementation: [https://github.com/HKUST-KnowComp/JWE/blob/master/src/word_analogy.py](https://github.com/HKUST-KnowComp/JWE/blob/master/src/word_analogy.py)


### Results

| System | Accuracy (capital) | Accuracy (state) | Accuracy (family) | Accuracy (total) |
| --- | --- | --- | --- | ---|
| [Yu et. al. (2017)](https://www.aclweb.org/anthology/D17-1027) (JWE)| 0.91 | 0.93 | 0.62 | 0.85 |
| [Yin et. al. (2016)](https://www.aclweb.org/anthology/D16-1100) (MGE) | 0.89 | 0.88 | 0.39 | 0.76 |
| CBOW (baseline) | 0.84 | 0.88 | 0.60 | 0.79 |


## Extrinsic evaluation-Chinese sentiment analysis
* This test measures how much the sentiment analysis task benefits from different word vectors.
* There is no agreed-upon baseline (e.g., sentiment classifier code), so it is difficult to compare across papers.
* Sentiment dataset available at [http://sentic.net/chinese-review-datasets.zip](http://sentic.net/chinese-review-datasets.zip) ([Peng et. al. (2018)](https://www.sciencedirect.com/science/article/abs/pii/S0950705118300972))
   * Consists of Chinese reviews in 4 domains: notebook, car, camera and phone
   * Binary classification task: reviews are either positive or negative
   * Does not have train/dev/test split.

| Test set | # positive reviews | # negative reviews |
| --- | --- | --- |
| Notebook | 417 | 206 |
| Car | 886 | 286 |
| Camera | 1,558 | 673 |
| Phone | 1,713 | 843 |

### Results

| System | Accuracy (notebook) | Accuracy (car) | Accuracy (camera) | Accuracy (phone) | Accuracy (overall) |
| --- | --- | --- | --- | ---| ---|
| [Sun et. al. (2019)](https://arxiv.org/pdf/1902.08795.pdf) (VCWE) | 80.95 | 85.59 | 83.93 | 84.38 | 88.92 |
| [Yu et. al. (2017)](https://www.aclweb.org/anthology/D17-1027) (JWE) | 77.78 | 78.81 | 81.70 | 81.64 | 85.13 |
| Baseline (skip-gram) | 69.84 | 77.12 | 80.80 | 81.25 | 86.65 |

## Extrinsic evaluation-Chinese name tagging
* This test measures how much the name tagging task benefits from different word vectors.
* There is no agreed-upon baseline (e.g., name tagging code), so it is difficult to compare across papers.
* This evaluation evaluates entity taggers on three types of entities: Person (PER), Location (LOC), and Organization (ORG): [Levow (2006)](http://acl-arc.comp.nus.edu.sg/archives/acl-arc-090501d4/data/pdf/anthology-PDF/W/W06/W06-0115.pdf)

| Test set | Size (words) | Genre |
| --- | --- | --- |
| SIGHAN 2006 NER MSRA | 100,000 | Newswire, Broadcast News, Weblog |

### Results

| System | F1 score |
| --- | --- |
| [Sun et. al. (2019)](https://arxiv.org/pdf/1902.08795.pdf) (VCWE) | 85.77 |
| [Yu et. al. (2017)](https://www.aclweb.org/anthology/D17-1027) (JWE)  | 85.30 |

### Resources

| Train set | Size (words) | Genre |
| --- | --- | --- |
| SIGHAN 2006 NER MSRA | 1.3M  | Newswire, Broadcast News, Weblog |

# Chinese Transliteration (中文翻译)
## Background

Transliteration translates proper names and technical terms across languages that use different alphabets and sound systems.

## Example input/output

Input:

```
约翰伍兹 (yue han wu zi)
```
Output:

```
John Woods
```

## Standard Metrics
- Word Accuracy in Top-1 (ACC)  measures correctness of the first transliteration candidate in a candidate list produced by a transliteration system.
- Fuzziness in Top-1 (Mean F-score).
- Mean Reciprocal Rank (MRR).
- MAP measures precision in the n-best candidates for i-th source name, for which reference transliterations are available.

# Chinese Text Classification
## Background

Text classification assigns tags or categories to text according to its topical content, typically training on labeled documents. Topics are sometimes broad and akin to genre (news, sports, arts) but sometimes as fine-grained as hashtags.

## Example input/output

Input:

```
[国足]有信心了 中国国奥队取得热身赛三连胜
```
Output:

```
Sports
```


## Standard Metrics
- Accuracy: the percentage of correctly classified samples.



# Chinese Text Summarization
## Background

Text summarization takes a long text document and creates a shorter text document that is a fluent and accurate summary of the longer text document.

## Example

Input:

```
 较早进入中国市场的星巴克， 是不少小资钟 情的品牌。相比在美国的平民形象，星巴克在中国就 显得“高端”得多。用料并无差别的一杯中杯美式咖 啡，在美国仅约合人民币12元，国内要卖21元，相当 于贵了75%。第一财经日报 
```

Output:

```
媒体称星巴克美式咖啡售价中国比美国 贵75%。
```

## Standard Metrics

ROUGE compares an automatically produced summary with human-produced, reference summaries.  ROUGE-1 records unigram overlap, ROUGE-2 bigram overlap, and ROUGE-L the longest common subsequence.   ROUGE can be computed over characters or words. [ROUGE将自动生成的摘要与人工生成的参考摘要进行比较。 ROUGE-1记录单字重叠，ROUGE-2记录大字重叠，ROUGE-L记录最长的共同序列。  ROUGE可以对字符或单词进行计算。]

Implementations
* [http://www.berouge.com/Pages/default.aspx](http://www.berouge.com/Pages/default.aspx) 
* [https://github.com/lancopku/superAE/blob/master/data/script/PythonROUGE.py](https://github.com/lancopku/superAE/blob/master/data/script/PythonROUGE.py) 

# Chinese Spelling Correction
## Background

A spelling corrector finds and correct typographical errors in text. These errors often occur between characters that are similar in appearance, pronunciation, or both.

纠正

## Example

Input:

```
1986年毕业于国防科技大学计算机应用专业，获学时学位。
```

Output:

```
1986年毕业于国防科技大学计算机应用专业，获学士学位。
(时 -> 士)
```

## Standard Metrics

Spelling correction performance is typically evaluated using **accuracy, precision, recall, and F1 score**. These metrics can be computed **at the character level or the sentence level**. **Detection and correction are typically evaluated separately**.

* Detection: all locations of incorrect characters in a given passage should be completely identical with the gold standard.  
* Correction: all locations and corresponding corrections of incorrect characters should be completely identical with the gold standard.


# Chinese Simplified/Traditional Conversion

## Background

Chinese Simplified/Traditional conversion converts simplified Chinese characters into traditional Chinese characters, and vice versa.


## Example

Input:

```
苟利国家生死以,岂因祸福避趋之.
```

Output: 

```
苟利國家生死以,豈因禍福避趨之.
```


## Standard Metrics

Accuracy 

## Standard Test Set

None, to our knowledge.


# Chinese Sentiment Analysis


## Background

Sentiment Analysis detects identifies and extracts subjective information from text.

## Example

Input:

```
总的感觉这台机器还不错，实用的有：阴阳历显示，时间与日期快速转换, 记事本等。
```

Output:

```
Positive
```

## Standard Metrics

Accuracy
  * The percentage of correctly classified samples on test set.

F1-score
  * Combination of precision and recall.
  * [Wiki Page](https://en.wikipedia.org/wiki/F1_score)





***

<!-- 插入图片 -->
![](https://cdn.jsdelivr.net/gh/xunhs/image_host/images/2020/8/pexels-aa-dil-2734404.jpg)