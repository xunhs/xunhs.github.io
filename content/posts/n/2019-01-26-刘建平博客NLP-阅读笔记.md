---
layout: post
cid: 93
title: 刘建平博客NLP-阅读笔记
slug: 93
date: 2019-01-26T08:49:28+00:00
updated: '2020/02/21 15:09:36'
status: publish
author: Ethan
categories:
  - 收藏
tags:
  - 阅读笔记
  - NLP
  - 刘建平
abbrlink: a28cba2f
---


<!-- Abstract -->
> 整理博客学习过程中一些笔记。

<!-- Abstract -->

<!--more-->

<!-- 正文内容 -->

### [文本挖掘的分词原理](https://www.cnblogs.com/pinard/p/6677078.html)
1. 分词的基本原理：利用语料库建立的统计概率，对于一个新的句子，我们就可以通过计算各种分词方法对应的联合分布概率，找到最大概率对应的分词方法，即为最优分词。
2. N元模型
3. 维特比算法与分词：维特比算法可以大大简化求出最优分词的时间
4. 常用分词工具：英文分词推荐使用nltk。对于中文分词，则推荐用结巴分词（jieba）

分词是文本挖掘的预处理的重要的一步，分词完成后，我们可以继续做一些其他的特征工程，比如向量化（vectorize），TF-IDF以及Hash trick，这些我们后面再讲。

### [向量化与Hash Trick](https://www.cnblogs.com/pinard/p/6688348.html)
1. 词袋模型(Bag of Words), 词集模型(Set of Words)
2. 词袋模型的三部曲：
   - 分词（tokenizing）
   - TF-IDF统计修订词特征值（counting）
   - 标准化（normalizing）

这里我们对向量化与它的特例Hash Trick做一个总结。在特征预处理的时候，我们什么时候用一般意义的向量化，什么时候用Hash Trick呢？标准也很简单。  
一般来说，只要词汇表的特征不至于太大，大到内存不够用，肯定是使用一般意义的向量化比较好。因为向量化的方法解释性很强，我们知道每一维特征对应哪一个词，进而我们还可以使用TF-IDF对各个词特征的权重修改，进一步完善特征的表示。  
而Hash Trick用大规模机器学习上，此时我们的词汇量极大，使用向量化方法内存不够用，而使用Hash Trick降维速度很快，降维后的特征仍然可以帮我们完成后续的分类和聚类工作。当然由于分布式计算框架的存在，其实一般我们不会出现内存不够的情况。因此，实际工作中我使用的都是特征向量化。  
向量化与Hash Trick就介绍到这里，下一篇我们讨论TF-IDF。分词是文本挖掘的预处理的重要的一步，分词完成后，我们可以继续做一些其他的特征工程，比如向量化（vectorize），TF-IDF以及Hash trick，这些我们后面再讲。


### [TF-IDF](https://www.cnblogs.com/pinard/p/6693230.html)

如果我们直接将统计词频后的19维特征做为文本分类的输入，会发现有一些问题。比如第一个文本，我们发现”come”,”China”和“Travel”各出现1次，而“to“出现了两次。似乎看起来这个文本与”to“这个特征更关系紧密。但是实际上”to“是一个非常普遍的词，几乎所有的文本都会用到，因此虽然它的词频为2，但是重要性却比词频为1的”China”和“Travel”要低的多。如果我们的向量化特征仅仅用词频表示就无法反应这一点。因此我们需要进一步的预处理来反应文本的这个特征，而这个预处理就是TF-IDF。IDF就是来帮助我们来反应这个词的重要性的，进而修正仅仅用词频表示的词特征值。  
TF-IDF是非常常用的文本挖掘预处理基本步骤，但是如果预处理中使用了Hash Trick，则一般就无法使用TF-IDF了，因为Hash Trick后我们已经无法得到哈希后的各特征的IDF的值。使用了IF-IDF并标准化以后，我们就可以使用各个文本的词特征向量作为文本的特征，进行分类或者聚类分析。  
当然TF-IDF不光可以用于文本挖掘，在信息检索等很多领域都有使用。因此值得好好的理解这个方法的思想。

### [中文文本挖掘预处理流程总结](https://www.cnblogs.com/pinard/p/6744056.html)
1. 中文文本挖掘预处理特点
2. 数据收集：主题爬虫—-ache，ache允许我们用关键字或者一个分类算法来过滤出我们需要的主题语料，比较强大。
3. 除去数据中非文本部分
4. 处理中文编码问题
5. 中文分词：可以帮jieba加入词汇
6. 引入停用词
7. 特征处理
8. 建立分析模型

### [潜在语义索引（LSI）](https://www.cnblogs.com/pinard/p/6805861.html)

LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。  
通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。  
k是我们假设的主题数，一般要比文本数少。  
主要的问题有：  
  1. SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。
  2. 主题值的选取对结果的影响非常大，很难选择合适的k值。
  3. LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。

对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。  
回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。

Gensim LSI实现：  
  - 中文：https://blog.csdn.net/ld326/article/details/78508162  
  - 英文：https://www.52nlp.cn/tag/lsi；官网：https://radimrehurek.com/gensim/wiki.html#id6

### [非负矩阵分解（NMF）](https://www.cnblogs.com/pinard/p/6812011.html)
NMF作为一个漂亮的矩阵分解方法，它可以很好的用于主题模型，并且使主题的结果有基于概率分布的解释性。但是NMF以及它的变种pLSA虽然可以从概率的角度解释了主题模型，却都只能对训练样本中的文本进行主题识别，而对不在样本中的文本是无法识别其主题的。根本原因在于NMF与pLSA这类主题模型方法没有考虑主题概率分布的先验知识，比如文本中出现体育主题的概率肯定比哲学主题的概率要高，这点来源于我们的先验知识，但是无法告诉NMF主题模型。而LDA主题模型则考虑到了这一问题，目前来说，绝大多数的文本主题模型都是使用LDA以及其变体。下一篇我们就来讨论LDA主题模型。

<!-- 正文内容 -->
***

<!-- 图片位置 -->


<!-- 图片位置 -->

