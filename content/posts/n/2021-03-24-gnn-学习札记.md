---
title: GNN-学习札记
author: Ethan
type: post
date: 2021-03-24T00:46:59+00:00
categories:
  - 收藏

---
> 图卷积神经网络学习笔记。记录常用数据集、任务及网络架构。

<!--more-->

---

主要参考链接：

  * <a href="https://docs.dgl.ai/en/latest/tutorials/models/index.html" data-type="URL" data-id="https://docs.dgl.ai/en/latest/tutorials/models/index.html">Paper Study with DGL</a>
  * <a href="https://github.com/dmlc/dgl/tree/master/examples" data-type="URL" data-id="https://github.com/dmlc/dgl/tree/master/examples">Official DGL Examples and Modules</a>
  * <a href="https://docs.dgl.ai/guide_cn/index.html" data-type="URL" data-id="https://docs.dgl.ai/guide_cn/index.html">DGL用户指南</a>
  * <a href="https://ogb.stanford.edu/docs/dataset_overview/" data-type="URL" data-id="https://ogb.stanford.edu/docs/dataset_overview/">OGB Dataset</a>
  * <a href="https://blog.csdn.net/szhwj123/article/details/111682822" data-type="URL" data-id="https://blog.csdn.net/szhwj123/article/details/111682822">图基准数据集(OGB)</a>

## 常用数据

### 引文网络（Cora、PubMed、Citeseer）

Cora为例：

Cora数据集由机器学习论文组成，是近年来图深度学习很喜欢使用的数据集。在数据集中，论文分为以下七类之一:基于案例、遗传算法、神经网络、概率方法、强化学习、规则学习、理论。论文的选择方式是，在最终语料库中，每篇论文引用或被至少一篇其他论文引用。整个语料库中有2708篇论文。在词干堵塞和去除词尾后，只剩下1433个独特的单词。文档频率小于10的所有单词都被删除。cora数据集包含1433个独特单词，所以特征是1433维。0和1描述的是每个单词在paper中是否存在。

数据集划分：Cora数据含有2708个样本，划分为**labeled data, test data and unlabeled data**.<figure class="wp-block-image size-large">

![](https://cdn.jsdelivr.net/gh/xunhs/image_host@master/PicX/image.5wfbkd1xnx8g.png)

#### 文件列表

  * ind.dataset_str.x => **训练**实例的特征向量，是`scipy.sparse.csr.csr_matrix`类对象，shape:(140, 1433)；140个样本，每类随机选择20个样本，一共包含7个类别，因此训练集含有140个样本（**即labeled 样本**）；

```python

with open("data/ind.cora.x", 'rb') as f:
    data = pkl.load(f, encoding='latin1')
    print(type(data))   #&lt;class 'scipy.sparse.csr.csr_matrix'&gt;
    print(data.shape)   #(140, 1433)-ind.cora.x是140行，1433列的
    print(data[1])
    nonzero=data.nonzero()
    print(nonzero)
    print(data.toarray())

-------
# print(data[1])
# 变量data是个scipy.sparse.csr.csr_matrix，类似稀疏矩阵，输出得到的是矩阵中非0的行列坐标及值
# (0, 19)   1.0
# (0, 88)   1.0
# (0, 149)  1.0
# (0, 212)  1.0
# (0, 233)  1.0
# (0, 332)  1.0
# (0, 336)  1.0
# (0, 359)  1.0
# (0, 472)  1.0
# (0, 507)  1.0
# (0, 548)  1.0
# ...
-------
# print(nonzero)
# 输出非零元素对应的行坐标和列坐标, nonzero是个tuple
# (array([  0,   0,   0, ..., 139, 139, 139], dtype=int32), array([  19,   81,  146, ..., 1263, 1274, 1393], dtype=int32))
-------
# print(data.toarray())
# [[0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]
#  [0. 0. 0. ... 0. 0. 0.]
#  ...
#  [0. 0. 0. ... 0. 1. 0.]
#  [0. 0. 0. ... 0. 0. 0.]
#  [0. 1. 0. ... 0. 0. 0.]]
```


  * ind.dataset_str.tx => **测试**实例的特征向量,shape:(1000, 1433)；随机选取1000个样本（**即test 样本**）；
  * ind.dataset\_str.allx => 有标签的+无标签训练实例的特征向量，是ind.dataset\_str.x的**超集**，shape:(1708, 1433)；排除测试样本中选取的1000个样本，选取的剩余样本（**即labeled + unlabeled 样本**）。
  * ind.dataset_str.y => 训练实例的标签，独热编码，numpy.ndarray类的实例，是numpy.ndarray对象，shape：(140, 7)

```python
with open("data/ind.cora.y", 'rb') as f:
    data = pkl.load(f, encoding='latin1')
    print(type(data))   # &lt;class 'numpy.ndarray'&gt;
    print(data.shape)   # (140, 7)
    print(data[1])      # [0 0 0 0 1 0 0]
```

  * ind.dataset_str.ty => 测试实例的标签，独热编码，numpy.ndarray类的实例,shape:(1000, 7)
  * ind.dataset\_str.ally => 对应于ind.dataset\_str.allx的标签，独热编码,shape:(1708, 7)
  * ind.dataset\_str.graph => 图数据，collections.defaultdict类的实例，格式为 {index：[index\_of\_neighbor\_nodes]}

```python
with open("data/ind.cora.graph", "rb") as f:
    data = pkl.load(f, encoding="latin1")
    print(type(data))  # &lt;class 'collections.defaultdict'&gt;
    print(data)

------
# {
#     0: [633, 1862, 2582],
#     1: [2, 652, 654],
#     2: [1986, 332, 1666, 1, 1454],
#     ...,
#     2706: [165, 2707, 1473, 169],
#     2707: [598, 165, 1473, 2706],
# }
```

  * ind.dataset_str.test.index => 测试实例的id，1000行(1708-2707)

<div class="wp-block-group">
  <div class="wp-block-group__inner-container">
    <h4 id="codecell0">
      节点分类任务中，一直迷惑的<strong>mask的index</strong>
    </h4>
    <ul>
      <li>
        idx_test = test_idx_range.tolist() #1708-2707，共1000个 => <strong>计算测试acc</strong>
      </li>
      <li>
        idx_train = range(len(y)) #0-139，共140个 => <strong>训练过程计算loss，用于反向传播</strong>
      </li>
      <li>
        idx_val = range(len(y), len(y)+500)#140-539，共500个 => <strong>训练过程计算acc</strong>
      </li>
    </ul>
    <h3>
      生物化学结构（PPI、NCI-1、NCI-109、MUTAG、QM9、Tox21）
    </h3>
    <p>
      参考：
    </p>
    <ul>
      <li>
        数据描述：https://www.jianshu.com/p/67137451b67f；https://my.oschina.net/chengsen/blog/4545191
      </li>
      <li>
        TUDataset：https://chrsmrrs.github.io/datasets/docs/datasets/
      </li>
    </ul>
  </div>
</div>

## 常用网络

#### MLP
Linear + BatchNorm1d
```python
class MLP(nn.Module):
    """MLP with linear output"""
    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):
        """MLP layers construction
        Paramters
        ---------
        num_layers: int
            The number of linear layers
        input_dim: int
            The dimensionality of input features
        hidden_dim: int
            The dimensionality of hidden units at ALL layers
        output_dim: int
            The number of classes for prediction
        """
        super(MLP, self).__init__()
        self.linear_or_not = True  # default is linear model
        self.num_layers = num_layers
        self.output_dim = output_dim

        if num_layers &lt; 1:
            raise ValueError("number of layers should be positive!")
        elif num_layers == 1:
            # Linear model
            self.linear = nn.Linear(input_dim, output_dim)
        else:
            # Multi-layer model
            self.linear_or_not = False
            self.linears = torch.nn.ModuleList()
            self.batch_norms = torch.nn.ModuleList()

            self.linears.append(nn.Linear(input_dim, hidden_dim))
            for layer in range(num_layers - 2):
                self.linears.append(nn.Linear(hidden_dim, hidden_dim))
            self.linears.append(nn.Linear(hidden_dim, output_dim))

            for layer in range(num_layers - 1):
                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))

    def forward(self, x):
        if self.linear_or_not:
            # If linear model
            return self.linear(x)
        else:
            # If MLP
            h = x
            for i in range(self.num_layers - 1):
                h = F.relu(self.batch_norms[i](self.linears[i](h)))
            return self.linears[-1](h)
```

#### GCN

#### GraphSAGE

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from dgl.nn.pytorch.conv import SAGEConv


class GraphSAGE(nn.Module):
    def __init__(
        self,
        in_feats,
        n_hidden,
        n_layers,
        out_feats,
        activation,
        dropout,
        aggregator_type,
    ):
        super(GraphSAGE, self).__init__()
        self.layers = nn.ModuleList()
        self.dropout = nn.Dropout(dropout)
        self.activation = activation

        # input layer
        self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type))
        # hidden layers
        for i in range(n_layers - 1):
            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type))
        # output layer
        self.layers.append(SAGEConv(n_hidden, out_feats, aggregator_type))

    def forward(self, graph, inputs):
        h = self.dropout(inputs)
        for l, layer in enumerate(self.layers):
            h = layer(graph, h)
            if l != len(self.layers) - 1:
                h = self.activation(h)
                h = self.dropout(h)
        return h


model = GraphSAGE(
    in_feats=dataset.dim_nfeats,
    n_hidden=32,
    n_layers=2,
    out_feats=dataset.gclasses,
    activation=F.relu,
    dropout=0.5,
    aggregator_type="gcn",
)
```

#### GIN

```python
"""
How Powerful are Graph Neural Networks
https:&#47;&#47;arxiv.org/abs/1810.00826
https://openreview.net/forum?id=ryGs6iA5Km
Author's implementation: https://github.com/weihua916/powerful-gnns
"""

from dgl.nn.pytorch.conv import GINConv
from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling


class ApplyNodeFunc(nn.Module):
    """Update the node feature hv with MLP, BN and ReLU."""
    def __init__(self, mlp):
        super(ApplyNodeFunc, self).__init__()
        self.mlp = mlp
        self.bn = nn.BatchNorm1d(self.mlp.output_dim)

    def forward(self, h):
        h = self.mlp(h)
        h = self.bn(h)
        h = F.relu(h)
        return h


class MLP(nn.Module):
    """MLP with linear output"""
    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):
        """MLP layers construction
        Paramters
        ---------
        num_layers: int
            The number of linear layers
        input_dim: int
            The dimensionality of input features
        hidden_dim: int
            The dimensionality of hidden units at ALL layers
        output_dim: int
            The number of classes for prediction
        """
        super(MLP, self).__init__()
        self.linear_or_not = True  # default is linear model
        self.num_layers = num_layers
        self.output_dim = output_dim

        if num_layers &lt; 1:
            raise ValueError("number of layers should be positive!")
        elif num_layers == 1:
            # Linear model
            self.linear = nn.Linear(input_dim, output_dim)
        else:
            # Multi-layer model
            self.linear_or_not = False
            self.linears = torch.nn.ModuleList()
            self.batch_norms = torch.nn.ModuleList()

            self.linears.append(nn.Linear(input_dim, hidden_dim))
            for layer in range(num_layers - 2):
                self.linears.append(nn.Linear(hidden_dim, hidden_dim))
            self.linears.append(nn.Linear(hidden_dim, output_dim))

            for layer in range(num_layers - 1):
                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))

    def forward(self, x):
        if self.linear_or_not:
            # If linear model
            return self.linear(x)
        else:
            # If MLP
            h = x
            for i in range(self.num_layers - 1):
                h = F.relu(self.batch_norms[i](self.linears[i](h)))
            return self.linears[-1](h)


class GIN(nn.Module):
    """GIN model"""
    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim,
                 output_dim, final_dropout, learn_eps, graph_pooling_type,
                 neighbor_pooling_type):
        """model parameters setting
        Paramters
        ---------
        num_layers: int
            The number of linear layers in the neural network
        num_mlp_layers: int
            The number of linear layers in mlps
        input_dim: int
            The dimensionality of input features
        hidden_dim: int
            The dimensionality of hidden units at ALL layers
        output_dim: int
            The number of classes for prediction
        final_dropout: float
            dropout ratio on the final linear layer
        learn_eps: boolean
            If True, learn epsilon to distinguish center nodes from neighbors
            If False, aggregate neighbors and center nodes altogether.
        neighbor_pooling_type: str
            how to aggregate neighbors (sum, mean, or max)
        graph_pooling_type: str
            how to aggregate entire nodes in a graph (sum, mean or max)
        """
        super(GIN, self).__init__()
        self.num_layers = num_layers
        self.learn_eps = learn_eps

        # List of MLPs
        self.ginlayers = torch.nn.ModuleList()
        self.batch_norms = torch.nn.ModuleList()

        for layer in range(self.num_layers - 1):
            if layer == 0:
                mlp = MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim)
            else:
                mlp = MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim)

            self.ginlayers.append(
                GINConv(ApplyNodeFunc(mlp), neighbor_pooling_type, 0, self.learn_eps))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # Linear function for graph poolings of output of each layer
        # which maps the output of different layers into a prediction score
        self.linears_prediction = torch.nn.ModuleList()

        for layer in range(num_layers):
            if layer == 0:
                self.linears_prediction.append(
                    nn.Linear(input_dim, output_dim))
            else:
                self.linears_prediction.append(
                    nn.Linear(hidden_dim, output_dim))

        self.drop = nn.Dropout(final_dropout)

        if graph_pooling_type == 'sum':
            self.pool = SumPooling()
        elif graph_pooling_type == 'mean':
            self.pool = AvgPooling()
        elif graph_pooling_type == 'max':
            self.pool = MaxPooling()
        else:
            raise NotImplementedError

    def forward(self, g, h):
        # list of hidden representation at each layer (including input)
        hidden_rep = [h]

        for i in range(self.num_layers - 1):
            h = self.ginlayers[i](g, h)
            h = self.batch_norms[i](h)
            h = F.relu(h)
            hidden_rep.append(h)

        score_over_layer = 0

        # perform pooling over all nodes in each graph in every layer
        for i, h in enumerate(hidden_rep):
            pooled_h = self.pool(g, h)
            score_over_layer += self.drop(self.linears_prediction[i](pooled_h))

        return score_over_layer



model = GIN(
    args.num_layers,
    args.num_mlp_layers,
    dataset.dim_nfeats,
    args.hidden_dim,
    dataset.gclasses,
    args.final_dropout,
    args.learn_eps,
    args.graph_pooling_type,
    args.neighbor_pooling_type,
).to(args.device)
```

## 任务

### 节点分类

对于图神经网络来说，最常见和被广泛使用的任务之一就是节点分类。 图数据中的训练、验证和测试集中的每个节点都具有从一组预定义的类别中分配的一个类别，即正确的标注。 节点回归任务也类似，训练、验证和测试集中的每个节点都被标注了一个正确的数字。为了对节点进行分类，图神经网络执行了<a rel="noreferrer noopener" href="https://docs.dgl.ai/guide_cn/message.html#guide-cn-message-passing" data-type="URL" data-id="https://docs.dgl.ai/guide_cn/message.html#guide-cn-message-passing" target="_blank">消息传递机制</a>，**利用节点自身的特征和其邻节点及边的特征来计算节点的隐藏表示**。 消息传递可以重复多轮，以利用更大范围的邻居信息。

#### cora数据集上节点分类

```python
"""
Inductive Representation Learning on Large Graphs
Paper: http://papers.nips.cc/paper/6703-inductive-representation-learning-on-large-graphs.pdf
Code: https://github.com/williamleif/graphsage-simple
Simple reference implementation of GraphSAGE.
"""
import argparse
import time

import dgl
import networkx as nx
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from dgl import DGLGraph
from dgl.data import load_data, register_data_args
from dgl.nn.pytorch.conv import SAGEConv

class GraphSAGE(nn.Module):
    def __init__(
        self,
        in_feats,
        n_hidden,
        n_classes,
        n_layers,
        activation,
        dropout,
        aggregator_type,
    ):
        super(GraphSAGE, self).__init__()
        self.layers = nn.ModuleList()
        self.dropout = nn.Dropout(dropout)
        self.activation = activation

        # input layer
        self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type))
        # hidden layers
        for i in range(n_layers - 1):
            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type))
        # output layer
        self.layers.append(
            SAGEConv(n_hidden, n_classes, aggregator_type)
        )  # activation None

    def forward(self, graph, inputs):
        h = self.dropout(inputs)
        for l, layer in enumerate(self.layers):
            h = layer(graph, h)
            if l != len(self.layers) - 1:
                h = self.activation(h)
                h = self.dropout(h)
        return h

def evaluate(model, graph, features, labels, nid):
    model.eval()
    with torch.no_grad():
        logits = model(graph, features)
        logits = logits[nid]
        labels = labels[nid]
        _, indices = torch.max(logits, dim=1)
        correct = torch.sum(indices == labels)
        return correct.item() * 1.0 / len(labels)


def main(args):
    # load and preprocess dataset
    data = load_data(args)
    g = data[0]
    features = g.ndata["feat"]
    labels = g.ndata["label"]
    train_mask = g.ndata["train_mask"]
    val_mask = g.ndata["val_mask"]
    test_mask = g.ndata["test_mask"]
    in_feats = features.shape[1]
    n_classes = data.num_classes
    n_edges = data.graph.number_of_edges()
    print(
        """----Data statistics------'
      #Edges %d
      #Classes %d
      #Train samples %d
      #Val samples %d
      #Test samples %d"""
        % (
            n_edges,
            n_classes,
            train_mask.int().sum().item(),
            val_mask.int().sum().item(),
            test_mask.int().sum().item(),
        )
    )

    if args.gpu &lt; 0:
        cuda = False
    else:
        cuda = True
        torch.cuda.set_device(args.gpu)
        features = features.cuda()
        labels = labels.cuda()
        train_mask = train_mask.cuda()
        val_mask = val_mask.cuda()
        test_mask = test_mask.cuda()
        print("use cuda:", args.gpu)

    train_nid = train_mask.nonzero().squeeze()
    val_nid = val_mask.nonzero().squeeze()
    test_nid = test_mask.nonzero().squeeze()

    # graph preprocess and calculate normalization factor
    g = dgl.remove_self_loop(g)
    n_edges = g.number_of_edges()
    if cuda:
        g = g.int().to(args.gpu)

    # create GraphSAGE model
    model = GraphSAGE(
        in_feats,
        args.n_hidden,
        n_classes,
        args.n_layers,
        F.relu,
        args.dropout,
        args.aggregator_type,
    )

    if cuda:
        model.cuda()

    # use optimizer
    optimizer = torch.optim.Adam(
        model.parameters(), lr=args.lr, weight_decay=args.weight_decay
    )

    # initialize graph
    dur = []
    for epoch in range(args.n_epochs):
        model.train()
        if epoch &gt;= 3:
            t0 = time.time()
        # forward
        logits = model(g, features)
        loss = F.cross_entropy(logits[train_nid], labels[train_nid])

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if epoch &gt;= 3:
            dur.append(time.time() - t0)

        acc = evaluate(model, g, features, labels, val_nid)
        print(
            "Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Accuracy {:.4f} | "
            "ETputs(KTEPS) {:.2f}".format(
                epoch, np.mean(dur), loss.item(), acc, n_edges / np.mean(dur) / 1000
            )
        )

    print()
    acc = evaluate(model, g, features, labels, test_nid)
    print("Test Accuracy {:.4f}".format(acc))


parser = argparse.ArgumentParser(description="GraphSAGE")
register_data_args(parser)
parser.add_argument("--dropout", type=float, default=0.5, help="dropout probability")
parser.add_argument("--gpu", type=int, default=-1, help="gpu")
parser.add_argument("--lr", type=float, default=1e-2, help="learning rate")
parser.add_argument(
    "--n-epochs", type=int, default=200, help="number of training epochs"
)
parser.add_argument(
    "--n-hidden", type=int, default=16, help="number of hidden gcn units"
)
parser.add_argument(
    "--n-layers", type=int, default=1, help="number of hidden gcn layers"
)
parser.add_argument(
    "--weight-decay", type=float, default=5e-4, help="Weight for L2 loss"
)
parser.add_argument(
    "--aggregator-type",
    type=str,
    default="gcn",
    help="Aggregator type: mean/gcn/pool/lstm",
)
# 设置参数
args = parser.parse_args(args=["--dataset", "cora", "--gpu", "0"])
print(args)

main(args)

------
# output
Epoch 00000 | Time(s) nan | Loss 1.9592 | Accuracy 0.0580 | ETputs(KTEPS) nan
Epoch 00001 | Time(s) nan | Loss 1.9487 | Accuracy 0.0580 | ETputs(KTEPS) nan
...
Epoch 00198 | Time(s) 0.0105 | Loss 0.3375 | Accuracy 0.7980 | ETputs(KTEPS) 1006.57
Epoch 00199 | Time(s) 0.0105 | Loss 0.3457 | Accuracy 0.7960 | ETputs(KTEPS) 1006.80

Test Accuracy 0.8220
```

### 边分类/回归

有时用户希望预测图中边的属性值，这种情况下，用户需要构建一个边分类/回归的模型。多层GNN同样也可以被用于计算任何节点的隐藏表示， 并**从边的两个端点的表示，通过计算得出对边属性的预测**。对一条边计算预测值最常见的情况是将预测表示为一个函数，**函数的输入为两个端点的表示， 输入还可以包括边自身的特征。**

#### 与节点分类在模型实现上的差别

节点分类模型中计算了节点的表示，那么用户只需要再编写一个`apply_edges()`方法计算边预测的组件即可进行边分类/回归任务。例如，**对于边回归任务，如果用户想为每条边计算一个分数，可按下面的代码对每一条边计算它的两端节点隐藏表示的点积来作为分数。**

```python
import dgl.function as fn
class DotProductPredictor(nn.Module):
    def forward(self, graph, h):
        # h是从5.1节的GNN模型中计算出的节点表示
        with graph.local_scope():
            graph.ndata['h'] = h
            graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))
            return graph.edata['score']
```

用户也可以使用**MLP(多层感知机)对每条边生成一个向量表示**(例如，作为一个未经过归一化的类别的分布)， 并在下游任务中使用。

```python
class MLPPredictor(nn.Module):
    def __init__(self, in_features, out_classes):
        super().__init__()
        self.W = nn.Linear(in_features * 2, out_classes)

    def apply_edges(self, edges):
        h_u = edges.src['h']
        h_v = edges.dst['h']
        score = self.W(torch.cat([h_u, h_v], 1))
        return {'score': score}

    def forward(self, graph, h):
        # h是从5.1节的GNN模型中计算出的节点表示
        with graph.local_scope():
            graph.ndata['h'] = h
            graph.apply_edges(self.apply_edges)
            return graph.edata['score']
```

构建给定计算节点和边上表示的模型。

```python
class Model(nn.Module):
    def __init__(self, in_features, hidden_features, out_features):
        super().__init__()
        self.sage = SAGE(in_features, hidden_features, out_features)
        self.pred = DotProductPredictor()
    def forward(self, g, x):
        h = self.sage(g, x)
        return self.pred(g, h)
```

### 链路预测

在某些场景中，用户可能**希望预测给定节点之间是否存在边**，这样的任务称作链路预测（或链接预测）&nbsp;任务。部分理论基础见<a href="https://docs.dgl.ai/guide_cn/training-link.html" data-type="URL" data-id="https://docs.dgl.ai/guide_cn/training-link.html">DGL-用户指南5.3链路预测</a>。

链路预测是一种定义在边上的任务，给定两个节点vi和vj,链路预测的目标是判断这两个节点之间是否有连接eij或它们之间的连接属于什么类别。链路预测和工业界联系十分紧密，很多推荐系统是基于链路预测的，知识图谱补全中对实体关系的预测也被认为是一种链路预测的问题。

参考资料：

  * <a href="https://liuchuang0059.github.io/2019/10/31/%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/" data-type="URL" data-id="https://liuchuang0059.github.io/2019/10/31/%E9%93%BE%E8%B7%AF%E9%A2%84%E6%B5%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E5%AD%A6%E4%B9%A0/">基础知识</a>
  * 评价指标：<a href="https://zhuanlan.zhihu.com/p/154147115" data-type="URL" data-id="https://zhuanlan.zhihu.com/p/154147115">R1</a>，<a href="https://www.neusncp.com/user/blog?id=220" data-type="URL" data-id="https://www.neusncp.com/user/blog?id=220">R2</a>

#### cora数据集上链路预测

Refer: https://docs.dgl.ai/en/latest/new-tutorial/4\_link\_predict.html

```python
import itertools

import dgl
import numpy as np
import scipy.sparse as sp
import torch
import torch.nn as nn
import torch.nn.functional as F
from dgl.nn.pytorch.conv import SAGEConv
from sklearn.metrics import roc_auc_score


'''
### Overview of LinkPrediction on Cora
#### Task: 
predicting whether a citation relationship, either citing or being cited, between two papers exists in a citation network.
#### Idea: 
formulates the link prediction problem as a binary classification problem
#### Steps:
 - Treat the edges in the graph as positive examples.
 - Sample a number of non-existent edges (i.e. node pairs with no edges between them) as negative examples.
 - Divide the positive examples and negative examples into a training set and a test set.
 - Evaluate the model with any binary classification metric such as Area Under Curve (AUC).
'''

import dgl.data

dataset = dgl.data.CoraGraphDataset()
g = dataset[0]


'''
### Prepare training and testing sets
randomly picks 10% of the edges for positive examples in the test set, and leave the rest for the training set. It then samples the same number of edges for negative examples in both sets.
'''

# Split edge set for training and testing
u, v = g.edges()  # source nodes and destination nodes
eids = np.arange(g.number_of_edges())  # edge ids
eids = np.random.permutation(eids)  # randomly shuffle the arrange
test_size = int(len(eids) * 0.1)  # the number of test sets
train_size = g.number_of_edges() - test_size  # .
test_pos_u, test_pos_v = (
    u[eids[:test_size]],
    v[eids[:test_size]],
)  # recording the position of test sets
train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]

# 1. Find all negative edges and 2. split them for training and testing
adj = sp.coo_matrix(
    (np.ones(len(u)), (u.numpy(), v.numpy()))
)  # construct a coordinate matrix, coo_matrix((data, (row indices, col indices)))
adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())
neg_u, neg_v = np.where(adj_neg != 0)

neg_eids = np.random.choice(len(neg_u), g.number_of_edges() // 2)
test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]
train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]


# When training, you will need to remove the edges in the test set from the original graph.
train_g = dgl.remove_edges(g, eids[:test_size])

# constructs the positive graph and the negative graph for the training set and the test set respectively
train_pos_g = dgl.graph((train_pos_u, train_pos_v), num_nodes=g.number_of_nodes())
train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())
test_pos_g = dgl.graph((test_pos_u, test_pos_v), num_nodes=g.number_of_nodes())
test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())


# building models

# produces a scalar score on each edge by concatenating the incident nodes’ features and passing it to an MLP
class MLPPredictor(nn.Module):
    def __init__(self, h_feats):
        super().__init__()
        self.W1 = nn.Linear(h_feats * 2, h_feats)
        self.W2 = nn.Linear(h_feats, 1)

    def apply_edges(self, edges):
        """
        Computes a scalar score for each edge of the given graph.

        Parameters
        ----------
        edges :
            Has three members ``src``, ``dst`` and ``data``, each of
            which is a dictionary representing the features of the
            source nodes, the destination nodes, and the edges
            themselves.

        Returns
        -------
        dict
            A dictionary of new edge features.
        """
        h = torch.cat([edges.src["h"], edges.dst["h"]], 1)
        return {"score": self.W2(F.relu(self.W1(h))).squeeze(1)}

    def forward(self, g, h):
        with g.local_scope():
            g.ndata["h"] = h
            g.apply_edges(self.apply_edges)
            return g.edata["score"]


# builds a model consisting of two GraphSAGE layers, each computes new node representations by aggregating neighbor information
class GraphSAGE(nn.Module):
    def __init__(
        self,
        in_feats,
        n_hidden,
        n_layers,
        activation,
        dropout,
        aggregator_type,
    ):
        super(GraphSAGE, self).__init__()
        self.layers = nn.ModuleList()
        self.dropout = nn.Dropout(dropout)
        self.activation = activation

        # input layer
        self.layers.append(SAGEConv(in_feats, n_hidden, aggregator_type))
        # hidden layers
        for i in range(n_layers - 1):
            self.layers.append(SAGEConv(n_hidden, n_hidden, aggregator_type))

    def forward(self, graph, inputs):
        h = self.dropout(inputs)
        for l, layer in enumerate(self.layers):
            h = layer(graph, h)
            if l != len(self.layers) - 1:
                h = self.activation(h)
                h = self.dropout(h)
        return h



model = GraphSAGE(
    in_feats=train_g.ndata["feat"].shape[1],
    n_hidden=32,
    n_layers=2,
    activation=F.relu,
    dropout=0.5,
    aggregator_type="gcn",
)
pred = MLPPredictor(32)


def compute_loss(pos_score, neg_score):
    scores = torch.cat([pos_score, neg_score])
    labels = torch.cat(
        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]
    )
    return F.binary_cross_entropy_with_logits(scores, labels)


def compute_auc(pos_score, neg_score):
    scores = torch.cat([pos_score, neg_score]).numpy()
    labels = torch.cat(
        [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]
    ).numpy()
    return roc_auc_score(labels, scores)


# calcuate hits@k - accuracy
# refer:
# 1. https://github.com/snap-stanford/ogb/blob/master/ogb/linkproppred/evaluate.py
# 2. https://github.com/snap-stanford/ogb/issues/105


def eval_hits(y_pred_pos, y_pred_neg, type_info="torch", K=100):
    """
    compute Hits@K
    For each positive target node, the negative target nodes are the same.
    y_pred_neg is an array.
    rank y_pred_pos[i] against y_pred_neg for each i
    """

    if len(y_pred_neg) &lt; K:
        return {"hits@{}".format(K): 1.0}

    if type_info == "torch":
        kth_score_in_negative_edges = torch.topk(y_pred_neg, K)[0][-1]
        hitsK = float(torch.sum(y_pred_pos &gt; kth_score_in_negative_edges).cpu()) / len(
            y_pred_pos
        )

    # type_info is numpy
    else:
        kth_score_in_negative_edges = np.sort(y_pred_neg)[-K]
        hitsK = float(np.sum(y_pred_pos &gt; kth_score_in_negative_edges)) / len(
            y_pred_pos
        )

    return hitsK


optimizer = torch.optim.Adam(
    itertools.chain(model.parameters(), pred.parameters()), lr=0.01
)


# training loooooooooooop
for e in range(301):
    # forward
    h = model(train_g, train_g.ndata["feat"])
    pos_score = pred(train_pos_g, h)
    neg_score = pred(train_neg_g, h)
    loss = compute_loss(pos_score, neg_score)

    # backward
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    with torch.no_grad():
        pos_score = pred(test_pos_g, h)
        neg_score = pred(test_neg_g, h)
        auc = compute_auc(pos_score, neg_score)

        test_hits = eval_hits(pos_score, neg_score)

    if e % 5 == 0:
        print(
            "In epoch {}, train loss: {}, test auc: {}, hits@100: {}".format(
                e,
                loss,
                auc,
                test_hits,
            )
        )
```

### 整图分类

许多场景中的图数据是由多个图组成，而不是单个的大图数据。例如不同类型的人群社区。 通过用图刻画同一社区里人与人间的友谊，可以得到多张用于分类的图。 在这个场景里，**整图分类模型可以识别社区的类型，即根据结构和整体信息对图进行分类**。整图分类与节点分类或链接预测的主要区别是：预测结果刻画了整个输入图的属性。 与之前的任务类似，用户还是在节点或边上进行消息传递。但不同的是，**整图分类任务还需要得到整个图的表示**。<figure class="wp-block-image size-large">

![](https://cdn.jsdelivr.net/gh/xunhs/image_host@master/PicX/image.57de2pzoq5g0.png)

#### GIN数据集整图分类

```python
import sys
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import argparse
from dgl.data import GINDataset

class Parser:
    def __init__(self, description, args_list=[]):
        """
        arguments parser
        """
        self.parser = argparse.ArgumentParser(description=description)
        self.args = None
        self._parse(args_list)

    def _parse(self, args_list):
        # dataset
        self.parser.add_argument(
            "--dataset",
            type=str,
            default="MUTAG",
            choices=["MUTAG", "COLLAB", "IMDBBINARY", "IMDBMULTI"],
            help="name of dataset (default: MUTAG)",
        )
        self.parser.add_argument(
            "--batch_size",
            type=int,
            default=32,
            help="batch size for training and validation (default: 32)",
        )
        self.parser.add_argument(
            "--fold_idx",
            type=int,
            default=0,
            help="the index(&lt;10) of fold in 10-fold validation.",
        )
        self.parser.add_argument("--filename", type=str, default="", help="output file")

        # device
        self.parser.add_argument(
            "--disable-cuda", action="store_true", help="Disable CUDA"
        )
        self.parser.add_argument(
            "--device", type=int, default=0, help="which gpu device to use (default: 0)"
        )

        # net
        self.parser.add_argument(
            "--num_layers", type=int, default=5, help="number of layers (default: 5)"
        )
        self.parser.add_argument(
            "--num_mlp_layers",
            type=int,
            default=2,
            help="number of MLP layers(default: 2). 1 means linear model.",
        )
        self.parser.add_argument(
            "--hidden_dim",
            type=int,
            default=64,
            help="number of hidden units (default: 64)",
        )

        # graph
        self.parser.add_argument(
            "--graph_pooling_type",
            type=str,
            default="sum",
            choices=["sum", "mean", "max"],
            help="type of graph pooling: sum, mean or max",
        )
        self.parser.add_argument(
            "--neighbor_pooling_type",
            type=str,
            default="sum",
            choices=["sum", "mean", "max"],
            help="type of neighboring pooling: sum, mean or max",
        )
        self.parser.add_argument(
            "--learn_eps", action="store_true", help="learn the epsilon weighting"
        )

        # learning
        self.parser.add_argument(
            "--seed", type=int, default=0, help="random seed (default: 0)"
        )
        self.parser.add_argument(
            "--epochs",
            type=int,
            default=350,
            help="number of epochs to train (default: 350)",
        )
        self.parser.add_argument(
            "--lr", type=float, default=0.01, help="learning rate (default: 0.01)"
        )
        self.parser.add_argument(
            "--final_dropout",
            type=float,
            default=0.5,
            help="final layer dropout (default: 0.5)",
        )

        # done
        self.args = self.parser.parse_args(args_list) # default args


args = Parser(description="GIN", args_list=[]).args
print(args)
# set up seeds, args.seed supported
torch.manual_seed(seed=args.seed)
np.random.seed(seed=args.seed)

is_cuda = not args.disable_cuda and torch.cuda.is_available()

if is_cuda:
    args.device = torch.device("cuda:" + str(args.device))
    torch.cuda.manual_seed_all(seed=args.seed)
else:
    args.device = torch.device("cpu")


import math
from torch.utils.data.sampler import SubsetRandomSampler
from sklearn.model_selection import StratifiedKFold
import dgl
from dgl.dataloading import GraphDataLoader

class GINDataLoader():
    def __init__(self,
                 dataset,
                 batch_size,
                 device,
                 collate_fn=None,
                 seed=0,
                 shuffle=True,
                 split_name='fold10',
                 fold_idx=0,
                 split_ratio=0.7):

        self.shuffle = shuffle
        self.seed = seed
        self.kwargs = {'pin_memory': True} if 'cuda' in device.type else {}

        labels = [l for _, l in dataset]

        if split_name == 'fold10':
            train_idx, valid_idx = self._split_fold10(
                labels, fold_idx, seed, shuffle)
        elif split_name == 'rand':
            train_idx, valid_idx = self._split_rand(
                labels, split_ratio, seed, shuffle)
        else:
            raise NotImplementedError()

        train_sampler = SubsetRandomSampler(train_idx)
        valid_sampler = SubsetRandomSampler(valid_idx)

        self.train_loader = GraphDataLoader(
            dataset, sampler=train_sampler,
            batch_size=batch_size, collate_fn=collate_fn, **self.kwargs)
        self.valid_loader = GraphDataLoader(
            dataset, sampler=valid_sampler,
            batch_size=batch_size, collate_fn=collate_fn, **self.kwargs)

    def train_valid_loader(self):
        return self.train_loader, self.valid_loader

    def _split_fold10(self, labels, fold_idx=0, seed=0, shuffle=True):
        ''' 10 flod '''
        assert 0 &lt;= fold_idx and fold_idx &lt; 10, print(
            "fold_idx must be from 0 to 9.")

        skf = StratifiedKFold(n_splits=10, shuffle=shuffle, random_state=seed)
        idx_list = []
        for idx in skf.split(np.zeros(len(labels)), labels):    # split(x, y)
            idx_list.append(idx)
        train_idx, valid_idx = idx_list[fold_idx]

        print(
            "train_set : test_set = %d : %d",
            len(train_idx), len(valid_idx))

        return train_idx, valid_idx

    def _split_rand(self, labels, split_ratio=0.7, seed=0, shuffle=True):
        num_entries = len(labels)
        indices = list(range(num_entries))
        np.random.seed(seed)
        np.random.shuffle(indices)
        split = int(math.floor(split_ratio * num_entries))
        train_idx, valid_idx = indices[:split], indices[split:]

        print(
            "train_set : test_set = %d : %d",
            len(train_idx), len(valid_idx))

        return train_idx, valid_idx


"""
How Powerful are Graph Neural Networks
https:&#47;&#47;arxiv.org/abs/1810.00826
https://openreview.net/forum?id=ryGs6iA5Km
Author's implementation: https://github.com/weihua916/powerful-gnns
"""

from dgl.nn.pytorch.conv import GINConv
from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling


class ApplyNodeFunc(nn.Module):
    """Update the node feature hv with MLP, BN and ReLU."""
    def __init__(self, mlp):
        super(ApplyNodeFunc, self).__init__()
        self.mlp = mlp
        self.bn = nn.BatchNorm1d(self.mlp.output_dim)

    def forward(self, h):
        h = self.mlp(h)
        h = self.bn(h)
        h = F.relu(h)
        return h


class MLP(nn.Module):
    """MLP with linear output"""
    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):
        """MLP layers construction
        Paramters
        ---------
        num_layers: int
            The number of linear layers
        input_dim: int
            The dimensionality of input features
        hidden_dim: int
            The dimensionality of hidden units at ALL layers
        output_dim: int
            The number of classes for prediction
        """
        super(MLP, self).__init__()
        self.linear_or_not = True  # default is linear model
        self.num_layers = num_layers
        self.output_dim = output_dim

        if num_layers &lt; 1:
            raise ValueError("number of layers should be positive!")
        elif num_layers == 1:
            # Linear model
            self.linear = nn.Linear(input_dim, output_dim)
        else:
            # Multi-layer model
            self.linear_or_not = False
            self.linears = torch.nn.ModuleList()
            self.batch_norms = torch.nn.ModuleList()

            self.linears.append(nn.Linear(input_dim, hidden_dim))
            for layer in range(num_layers - 2):
                self.linears.append(nn.Linear(hidden_dim, hidden_dim))
            self.linears.append(nn.Linear(hidden_dim, output_dim))

            for layer in range(num_layers - 1):
                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))

    def forward(self, x):
        if self.linear_or_not:
            # If linear model
            return self.linear(x)
        else:
            # If MLP
            h = x
            for i in range(self.num_layers - 1):
                h = F.relu(self.batch_norms[i](self.linears[i](h)))
            return self.linears[-1](h)


class GIN(nn.Module):
    """GIN model"""
    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim,
                 output_dim, final_dropout, learn_eps, graph_pooling_type,
                 neighbor_pooling_type):
        """model parameters setting
        Paramters
        ---------
        num_layers: int
            The number of linear layers in the neural network
        num_mlp_layers: int
            The number of linear layers in mlps
        input_dim: int
            The dimensionality of input features
        hidden_dim: int
            The dimensionality of hidden units at ALL layers
        output_dim: int
            The number of classes for prediction
        final_dropout: float
            dropout ratio on the final linear layer
        learn_eps: boolean
            If True, learn epsilon to distinguish center nodes from neighbors
            If False, aggregate neighbors and center nodes altogether.
        neighbor_pooling_type: str
            how to aggregate neighbors (sum, mean, or max)
        graph_pooling_type: str
            how to aggregate entire nodes in a graph (sum, mean or max)
        """
        super(GIN, self).__init__()
        self.num_layers = num_layers
        self.learn_eps = learn_eps

        # List of MLPs
        self.ginlayers = torch.nn.ModuleList()
        self.batch_norms = torch.nn.ModuleList()

        for layer in range(self.num_layers - 1):
            if layer == 0:
                mlp = MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim)
            else:
                mlp = MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim)

            self.ginlayers.append(
                GINConv(ApplyNodeFunc(mlp), neighbor_pooling_type, 0, self.learn_eps))
            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))

        # Linear function for graph poolings of output of each layer
        # which maps the output of different layers into a prediction score
        self.linears_prediction = torch.nn.ModuleList()

        for layer in range(num_layers):
            if layer == 0:
                self.linears_prediction.append(
                    nn.Linear(input_dim, output_dim))
            else:
                self.linears_prediction.append(
                    nn.Linear(hidden_dim, output_dim))

        self.drop = nn.Dropout(final_dropout)

        if graph_pooling_type == 'sum':
            self.pool = SumPooling()
        elif graph_pooling_type == 'mean':
            self.pool = AvgPooling()
        elif graph_pooling_type == 'max':
            self.pool = MaxPooling()
        else:
            raise NotImplementedError

    def forward(self, g, h):
        # list of hidden representation at each layer (including input)
        hidden_rep = [h]

        for i in range(self.num_layers - 1):
            h = self.ginlayers[i](g, h)
            h = self.batch_norms[i](h)
            h = F.relu(h)
            hidden_rep.append(h)

        score_over_layer = 0

        # perform pooling over all nodes in each graph in every layer
        for i, h in enumerate(hidden_rep):
            pooled_h = self.pool(g, h)
            score_over_layer += self.drop(self.linears_prediction[i](pooled_h))

        return score_over_layer


dataset = GINDataset(args.dataset, not args.learn_eps)
trainloader, validloader = GINDataLoader(
    dataset,
    batch_size=args.batch_size,
    device=args.device,
    seed=args.seed,
    shuffle=True,
    split_name="fold10",
    fold_idx=args.fold_idx,
).train_valid_loader()
# or split_name='rand', split_ratio=0.7


model = GIN(
    args.num_layers,
    args.num_mlp_layers,
    dataset.dim_nfeats,
    args.hidden_dim,
    dataset.gclasses,
    args.final_dropout,
    args.learn_eps,
    args.graph_pooling_type,
    args.neighbor_pooling_type,
).to(args.device)
criterion = nn.CrossEntropyLoss()  # defaul reduce is true
optimizer = optim.Adam(model.parameters(), lr=args.lr)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)



def train(args, net, trainloader, optimizer, criterion, epoch):
    net.train()

    running_loss = 0
    total_iters = len(trainloader)

    for graphs, labels in trainloader:
        # batch graphs will be shipped to device in forward part of model
        labels = labels.to(args.device)
        graphs = graphs.to(args.device)
        feat = graphs.ndata.pop('attr')
        outputs = net(graphs, feat)

        loss = criterion(outputs, labels)
        running_loss += loss.item()

        # backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


    # the final batch will be aligned
    running_loss = running_loss / total_iters

    return running_loss


def eval_net(args, net, dataloader, criterion):
    net.eval()

    total = 0
    total_loss = 0
    total_correct = 0

    for data in dataloader:
        graphs, labels = data
        graphs = graphs.to(args.device)
        labels = labels.to(args.device)
        feat = graphs.ndata.pop('attr')
        total += len(labels)
        outputs = net(graphs, feat)
        _, predicted = torch.max(outputs.data, 1)

        total_correct += (predicted == labels.data).sum().item()
        loss = criterion(outputs, labels)
        # crossentropy(reduce=True) for default
        total_loss += loss.item() * len(labels)

    loss, acc = 1.0*total_loss / total, 1.0*total_correct / total

    net.train()

    return loss, acc


for epoch in range(args.epochs):

    train(args, model, trainloader, optimizer, criterion, epoch)
    scheduler.step()
    train_loss, train_acc = eval_net(args, model, trainloader, criterion)
    valid_loss, valid_acc = eval_net(args, model, validloader, criterion)

    print(
        "Epoch: {}, train set - average loss: {:.4f}, accuracy: {:.0f}%, valid set - average loss: {:.4f}, accuracy: {:.0f}%".format(
            epoch, train_loss, 100.0 * train_acc, valid_loss, 100.0 * valid_acc
        )
    )
```

---
![](https://cdn.jsdelivr.net/gh/xunhs/image_host@master/PicX/pexels-ryutaro-tsukata-6249230.14nl1zyfcg4g.jpg)